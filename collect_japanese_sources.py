#!/usr/bin/env python3
"""æ—¥æœ¬èªã‚½ãƒ¼ã‚¹ç‰¹åŒ–ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³"""

import sys
import asyncio
import requests
import feedparser
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any
import time
import re

# Add src to path
project_root = Path(__file__).parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

from src.config.settings import Settings
from src.models.article import Article, TechnicalMetadata, BusinessMetadata, ImplementationCost
from src.generators.static_site_generator import StaticSiteGenerator

class JapaneseSourceCollector:
    """æ—¥æœ¬èªã‚½ãƒ¼ã‚¹å°‚ç”¨ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.sources = {
            # å‹•ä½œç¢ºèªæ¸ˆã¿æ—¥æœ¬èªã‚½ãƒ¼ã‚¹
            "itmedia_ai": {
                "url": "https://rss.itmedia.co.jp/rss/2.0/ait.xml",
                "alternative_url": "https://www.itmedia.co.jp/ait/",
                "tier": 1,
                "source_name": "ITmedia AI",
                "category": "japanese"
            },
            # è¿½åŠ ã®æ—¥æœ¬èªã‚½ãƒ¼ã‚¹ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
            "itmedia_news": {
                "url": "https://rss.itmedia.co.jp/rss/2.0/news_technology.xml",
                "alternative_url": "https://www.itmedia.co.jp/news/technology/",
                "tier": 1,
                "source_name": "ITmedia ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼",
                "category": "japanese"
            },
            "pc_watch": {
                "url": "https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf",
                "alternative_url": "https://pc.watch.impress.co.jp/",
                "tier": 1,
                "source_name": "PC Watch",
                "category": "japanese"
            },
            "internet_watch": {
                "url": "https://internet.watch.impress.co.jp/data/rss/1.0/iw/feed.rdf",
                "alternative_url": "https://internet.watch.impress.co.jp/",
                "tier": 1,
                "source_name": "INTERNET Watch",
                "category": "japanese"
            },
            "japan_cnet": {
                "url": "https://feeds.japan.cnet.com/rss/cnet/all.rdf",
                "alternative_url": "https://japan.cnet.com/",
                "tier": 1,
                "source_name": "CNET Japan",
                "category": "japanese"
            },
            "gizmodo_jp": {
                "url": "https://www.gizmodo.jp/index.xml",
                "alternative_url": "https://www.gizmodo.jp/",
                "tier": 1,
                "source_name": "Gizmodo Japan",
                "category": "japanese"
            },
            "techcrunch_jp": {
                "url": "https://jp.techcrunch.com/feed/",
                "alternative_url": "https://jp.techcrunch.com/",
                "tier": 1,
                "source_name": "TechCrunch Japan",
                "category": "japanese"
            },
            # ä»£æ›¿URLå½¢å¼ã®ã‚½ãƒ¼ã‚¹
            "mynavi_tech": {
                "url": "https://news.mynavi.jp/rss/techplus",
                "alternative_url": "https://news.mynavi.jp/techplus/",
                "tier": 2,
                "source_name": "ãƒã‚¤ãƒŠãƒ“ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ãƒƒã‚¯+",
                "category": "japanese"
            }
        }
    
    def test_feed_access(self, source_config: Dict) -> bool:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ"""
        try:
            headers = {
                'User-Agent': 'DailyAINews/1.0 (Educational Project)',
                'Accept': 'application/rss+xml, application/xml, text/xml'
            }
            
            response = requests.get(source_config['url'], headers=headers, timeout=10)
            if response.status_code == 200:
                return True
            
            # ä»£æ›¿URLè©¦è¡Œ
            if 'alternative_url' in source_config:
                response = requests.get(source_config['alternative_url'], headers=headers, timeout=10)
                return response.status_code == 200
                
            return False
        except:
            return False
    
    def collect_from_feed(self, feed_config: Dict, max_articles: int = 3) -> List[Article]:
        """æ—¥æœ¬èªãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹åé›†"""
        articles = []
        
        try:
            print(f"ğŸ“¡ æ—¥æœ¬èªã‚½ãƒ¼ã‚¹åé›†ä¸­: {feed_config['source_name']}...")
            
            # ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ
            if not self.test_feed_access(feed_config):
                print(f"âš ï¸ ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯: {feed_config['source_name']}")
                return articles
            
            headers = {
                'User-Agent': 'DailyAINews/1.0 (Educational Project)',
                'Accept': 'application/rss+xml, application/xml, text/xml',
                'Accept-Language': 'ja,en;q=0.9'
            }
            
            response = requests.get(feed_config['url'], headers=headers, timeout=15)
            
            if response.status_code != 200:
                print(f"âš ï¸ HTTP {response.status_code}: {feed_config['source_name']}")
                return articles
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç¢ºèª
            if 'charset' in response.headers.get('content-type', ''):
                encoding = response.headers['content-type'].split('charset=')[-1]
                response.encoding = encoding
            
            feed = feedparser.parse(response.content)
            
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                print(f"âš ï¸ ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãªã—: {feed_config['source_name']}")
                return articles
            
            print(f"ğŸ“„ {len(feed.entries)}å€‹ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’ç™ºè¦‹")
            
            for entry in feed.entries[:max_articles]:
                try:
                    # æ—¥ä»˜è§£æ
                    pub_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        pub_date = datetime(*entry.updated_parsed[:6])
                    else:
                        pub_date = datetime.now()
                    
                    # å¤ã„è¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if pub_date < datetime.now() - timedelta(days=60):
                        continue
                    
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
                    content = ""
                    if hasattr(entry, 'summary'):
                        content = entry.summary
                    elif hasattr(entry, 'description'):
                        content = entry.description
                    elif hasattr(entry, 'content'):
                        if isinstance(entry.content, list) and len(entry.content) > 0:
                            content = entry.content[0].get('value', '')
                        else:
                            content = str(entry.content)
                    
                    # HTMLã‚¿ã‚°ã‚’å‰Šé™¤
                    content = re.sub(r'<[^>]+>', '', content)
                    content = content.strip()
                    
                    # çŸ­ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if len(content) < 50:
                        continue
                    
                    # AIé–¢é€£ãƒã‚§ãƒƒã‚¯
                    ai_keywords = ['AI', 'ai', 'äººå·¥çŸ¥èƒ½', 'æ©Ÿæ¢°å­¦ç¿’', 'ML', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'ChatGPT', 'GPT']
                    content_and_title = (entry.title + " " + content).lower()
                    
                    if not any(keyword.lower() in content_and_title for keyword in ai_keywords):
                        continue
                    
                    article = Article(
                        id=f"{feed_config['source_name'].lower().replace(' ', '_').replace('.', '_')}_{hash(entry.link) % 10000}",
                        title=entry.title,
                        url=entry.link,
                        source=feed_config['source_name'],
                        source_tier=feed_config['tier'],
                        published_date=pub_date,
                        content=content,
                        tags=['japanese', 'ai']
                    )
                    
                    articles.append(article)
                    print(f"  âœ… åé›†: {entry.title[:40]}...")
                    
                except Exception as e:
                    print(f"  âš ï¸ ã‚¨ãƒ³ãƒˆãƒªãƒ¼å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}")
                    continue
            
            print(f"âœ… {feed_config['source_name']}: {len(articles)}è¨˜äº‹åé›†å®Œäº†")
            
        except Exception as e:
            print(f"âŒ {feed_config['source_name']}åé›†ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}")
        
        return articles
    
    def collect_all_japanese(self) -> List[Article]:
        """å…¨æ—¥æœ¬èªã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†"""
        all_articles = []
        
        print("ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªAIãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹")
        print("-" * 40)
        
        for source_key, config in self.sources.items():
            articles = self.collect_from_feed(config)
            all_articles.extend(articles)
            time.sleep(2)  # ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„
        
        # é‡è¤‡å‰Šé™¤
        unique_articles = []
        seen_urls = set()
        seen_titles = set()
        
        for article in all_articles:
            # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯
            if article.url in seen_urls:
                continue
            
            # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
            title_key = article.title.lower().replace(' ', '').replace('ã€€', '')[:30]
            if title_key in seen_titles:
                continue
            
            seen_urls.add(article.url)
            seen_titles.add(title_key)
            unique_articles.append(article)
        
        print(f"\nğŸ“Š åé›†çµæœ:")
        print(f"  â€¢ ç·è¨˜äº‹æ•°: {len(all_articles)}")
        print(f"  â€¢ é‡è¤‡é™¤å»å¾Œ: {len(unique_articles)}")
        
        return unique_articles

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸŒ¸ æ—¥æœ¬èªAIãƒ‹ãƒ¥ãƒ¼ã‚¹ç‰¹åŒ–ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
    print("=" * 50)
    
    settings = Settings()
    collector = JapaneseSourceCollector()
    
    # æ—¥æœ¬èªè¨˜äº‹åé›†
    japanese_articles = collector.collect_all_japanese()
    
    if not japanese_articles:
        print("âŒ æ—¥æœ¬èªè¨˜äº‹ãŒåé›†ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return False
    
    # ç°¡å˜ãªè©•ä¾¡
    for article in japanese_articles:
        content_lower = (article.title + " " + article.content).lower()
        
        # æ—¥æœ¬èªç‰¹æœ‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        tech_jp_keywords = ['æŠ€è¡“', 'é–‹ç™º', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°', 'ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢', 'å®Ÿè£…']
        business_jp_keywords = ['ãƒ“ã‚¸ãƒã‚¹', 'ä¼æ¥­', 'å°å…¥', 'æ´»ç”¨', 'åŠ¹æœ', 'å£²ä¸Š']
        
        tech_score = 0.5 + (sum(1 for k in tech_jp_keywords if k in content_lower) * 0.1)
        business_score = 0.5 + (sum(1 for k in business_jp_keywords if k in content_lower) * 0.1)
        
        article.evaluation = {
            "engineer": {"total_score": min(1.0, tech_score)},
            "business": {"total_score": min(1.0, business_score)}
        }
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        article.technical = TechnicalMetadata(
            implementation_ready=any(k in content_lower for k in ['github', 'ã‚³ãƒ¼ãƒ‰', 'å®Ÿè£…']),
            code_available=any(k in content_lower for k in ['github', 'ã‚³ãƒ¼ãƒ‰']),
            reproducibility_score=0.7
        )
        
        article.business = BusinessMetadata(
            market_size="æ—¥æœ¬å¸‚å ´",
            growth_rate=business_score * 100,
            implementation_cost=ImplementationCost.MEDIUM
        )
    
    # ã‚½ãƒ¼ãƒˆ
    japanese_articles.sort(key=lambda x: (x.evaluation["engineer"]["total_score"] + x.evaluation["business"]["total_score"]) / 2, reverse=True)
    
    print(f"\nğŸ† ãƒˆãƒƒãƒ—æ—¥æœ¬èªè¨˜äº‹:")
    for i, article in enumerate(japanese_articles[:3]):
        eng_score = article.evaluation["engineer"]["total_score"]
        bus_score = article.evaluation["business"]["total_score"]
        print(f"  {i+1}. {article.title}")
        print(f"     ã‚½ãƒ¼ã‚¹: {article.source}")
        print(f"     ã‚¹ã‚³ã‚¢: ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ {eng_score:.2f} | ãƒ“ã‚¸ãƒã‚¹ {bus_score:.2f}")
        print()
    
    # æ—¢å­˜è¨˜äº‹ã¨çµ±åˆã—ã¦ã‚µã‚¤ãƒˆç”Ÿæˆ
    try:
        generator = StaticSiteGenerator(settings)
        
        docs_dir = Path("docs")
        docs_dir.mkdir(exist_ok=True)
        
        # CSS/JSç”Ÿæˆ
        css_content = generator.assets.generate_css()
        (docs_dir / "styles.css").write_text(css_content, encoding='utf-8')
        
        js_content = generator.assets.generate_javascript()
        (docs_dir / "script.js").write_text(js_content, encoding='utf-8')
        
        # HTMLç”Ÿæˆ
        html_generator = generator.html_generator
        processed_articles = html_generator._process_articles(japanese_articles, "engineer")
        summary_stats = html_generator._generate_summary_stats(japanese_articles)
        
        articles_html = html_generator._render_articles_grid(processed_articles, "engineer")
        filters_html = html_generator._create_interactive_filters(html_generator._extract_filter_options(japanese_articles))
        stats_html = html_generator._render_summary_stats(summary_stats)
        
        dashboard_template = html_generator.template_engine.load_template("dashboard.html")
        dashboard_content = html_generator.template_engine.render(dashboard_template, {
            "articles": articles_html,
            "filters": filters_html,
            "summary_stats": stats_html
        })
        
        page_content = html_generator._generate_complete_page(
            dashboard_content,
            title="Daily AI News - æ—¥æœ¬èªAIãƒ‹ãƒ¥ãƒ¼ã‚¹ç‰¹åŒ–",
            description="æ—¥æœ¬èªAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å¤šå±¤è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ",
            persona="engineer"
        )
        
        index_file = docs_dir / "index.html"
        index_file.write_text(page_content, encoding='utf-8')
        
        print(f"âœ… æ—¥æœ¬èªAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆç”Ÿæˆå®Œäº†!")
        print(f"ğŸ“‚ å ´æ‰€: {index_file.absolute()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚µã‚¤ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    
    if success:
        print("\nğŸŒ æ—¥æœ¬èªAIãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ãƒ»ã‚µã‚¤ãƒˆç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    else:
        print("\nâš ï¸ å®Ÿè¡Œã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")
    
    print(f"\nğŸ’¡ ã‚µã‚¤ãƒˆã‚’é–‹ãã«ã¯:")
    print(f"  C:\\Users\\yoshitaka\\new-ai-news-site\\docs\\index.html")