[
  {
    "id": "rss_373638a6",
    "title": "The Math You Need to Pan and Tilt 360° Images | Towards Data Science",
    "url": "https://towardsdatascience.com/math-to-tilt-pan-360-images/",
    "source": "Towards Data Science",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Panning a spherical image is just a horizontal roll, but tilting it vertically is much trickier. Let's see the math!",
    "sourceDomain": "towardsdatascience.com"
  },
  {
    "id": "rss_a7d52f82",
    "title": "A Brief History of GPT Through Papers | Towards Data Science",
    "url": "https://towardsdatascience.com/a-brief-history-of-gpt-through-papers/",
    "source": "Towards Data Science",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Language models are becoming really good. But where did they come from?",
    "sourceDomain": "towardsdatascience.com"
  },
  {
    "id": "rss_a914993f",
    "title": "Time Series Forecasting Made Simple (Part 4.1): Understanding Stationarity in a Time Series | Towards Data Science",
    "url": "https://towardsdatascience.com/time-series-forecasting-made-simple-part-4-1-understanding-stationarity-in-a-time-series/",
    "source": "Towards Data Science",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "An intuitive guide to stationarity in a time series",
    "sourceDomain": "towardsdatascience.com"
  },
  {
    "id": "rss_2f6ac00a",
    "title": "Everything I Studied to Become a Machine Learning Engineer (No CS Background) | Towards Data Science",
    "url": "https://towardsdatascience.com/everything-i-studied-to-become-a-machine-learning-engineer-no-cs-background/",
    "source": "Towards Data Science",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "The books, courses, and resources I used in my journey.",
    "sourceDomain": "towardsdatascience.com"
  },
  {
    "id": "rss_b8ed50f8",
    "title": "Get AI-Ready: How to Prepare for a World of Agentic AI as Tech Professionals | Towards Data Science",
    "url": "https://towardsdatascience.com/getting-ai-ready-preparing-for-a-world-of-agentic-ai-as-tech-professionals/",
    "source": "Towards Data Science",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Explore how Agentic AI is reshaping the tech careers, from data to decision-making, and how professionals can prepare for the future of work",
    "sourceDomain": "towardsdatascience.com"
  },
  {
    "id": "rss_dae6a4dd",
    "title": "Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum",
    "url": "https://arxiv.org/abs/2508.18673",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often limited by the use of randomly or manually selected examples. These examples fail to account for both model-specific knowledge distributions and the intrinsic complexity of the tasks, resulting in suboptimal and unstable m…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae6a4a1",
    "title": "Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models",
    "url": "https://arxiv.org/abs/2508.18655",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "With the development of speech large language models (speech LLMs), users can now interact directly with assistants via speech. However, most existing models simply convert the response content into speech without fully understanding the rich emotional and paralinguistic cues embedded in the user…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae6a49d",
    "title": "Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models",
    "url": "https://arxiv.org/abs/2508.18651",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Grounding responses in external knowledge represents an effective strategy for mitigating hallucinations in Large Language Models (LLMs). However, current LLMs struggle to seamlessly integrate knowledge while simultaneously maintaining faithfulness (or fidelity) and expressiveness, capabilities t…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae6a485",
    "title": "Thinking Before You Speak: A Proactive Test-time Scaling Approach",
    "url": "https://arxiv.org/abs/2508.18648",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Large Language Models (LLMs) often exhibit deficiencies with complex reasoning tasks, such as maths, which we attribute to the discrepancy between human reasoning patterns and those presented in the LLMs' training data. When dealing with complex problems, humans tend to think carefully before exp…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae6a40a",
    "title": "Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models",
    "url": "https://arxiv.org/abs/2508.18609",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Large language models (LLMs) present significant deployment challenges due to their scale, with post-training quantization (PTQ) emerging as a practical compression solution. However, a comprehensive understanding of how PTQ precisely impacts diverse LLM knowledge capabilities remains elusive, an…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae6a408",
    "title": "A New NMT Model for Translating Clinical Texts from English to Spanish",
    "url": "https://arxiv.org/abs/2508.18607",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Translating electronic health record (EHR) narratives from English to Spanish is a clinically important yet challenging task due to the lack of a parallel-aligned corpus and the abundant unknown words contained. To address such challenges, we propose \\textbf{NOOV} (for No OOV), a new neural machi…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae6a15f",
    "title": "What do language models model? Transformers, automata, and the format of thought",
    "url": "https://arxiv.org/abs/2508.18598",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "What do large language models actually model? Do they tell us something about human capacities, or are they models of the corpus we've trained them on? I give a non-deflationary defence of the latter position. Cognitive science tells us that linguistic capabilities in humans rely supralinear form…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae6a103",
    "title": "The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation",
    "url": "https://arxiv.org/abs/2508.18569",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Visual metaphor generation is a challenging task that aims to generate an image given an input text metaphor. Inherently, it needs language understanding to bind a source concept with a target concept, in a way that preserves meaning while ensuring visual coherence. We propose a self-evaluating v…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae6a0c5",
    "title": "COMET-poly: Machine Translation Metric Grounded in Other Candidates",
    "url": "https://arxiv.org/abs/2508.18549",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Automated metrics for machine translation attempt to replicate human judgment. Unlike humans, who often assess a translation in the context of multiple alternatives, these metrics typically consider only the source sentence and a single translation. This discrepancy in the evaluation setup may ne…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae69d5b",
    "title": "Principled Detection of Hallucinations in Large Language Models via Multiple Testing",
    "url": "https://arxiv.org/abs/2508.18473",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "While Large Language Models (LLMs) have emerged as powerful foundational models to solve a variety of tasks, they have also been shown to be prone to hallucinations, i.e., generating responses that sound confident but are actually incorrect or even nonsensical. In this work, we formulate the prob…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae69d3f",
    "title": "Integrating gender inclusivity into large language models via instruction tuning",
    "url": "https://arxiv.org/abs/2508.18466",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Imagine a language with masculine, feminine, and neuter grammatical genders, yet, due to historical and political conventions, masculine forms are predominantly used to refer to men, women and mixed-gender groups. This is the reality of contemporary Polish. A social consequence of this unfair lin…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae69cff",
    "title": "How Reliable are LLMs for Reasoning on the Re-ranking task?",
    "url": "https://arxiv.org/abs/2508.18444",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "With the improving semantic understanding capability of Large Language Models (LLMs), they exhibit a greater awareness and alignment with human values, but this comes at the cost of transparency. Although promising results are achieved via experimental analysis, an in-depth understanding of the L…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae69c86",
    "title": "Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering",
    "url": "https://arxiv.org/abs/2508.18407",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "A majority of recent work in AI assesses models' generalization capabilities through the lens of performance on out-of-distribution (OOD) datasets. Despite their practicality, such evaluations build upon a strong assumption: that OOD evaluations can capture and reflect upon possible failures in a…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae699da",
    "title": "Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning",
    "url": "https://arxiv.org/abs/2508.18395",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Probabilistic decoding in Large Language Models (LLMs) often yields inconsistent outputs, particularly on complex or long-form questions. Self-Consistency (SC) mitigates this for short-form QA by majority voting over exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram Con…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae699bd",
    "title": "Integral Transformer: Denoising Attention, Not Too Much Not Too Little",
    "url": "https://arxiv.org/abs/2508.18387",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Softmax self-attention often assigns disproportionate weight to semantically uninformative tokens such as special tokens and punctuation, a phenomenon known as attention noise. While recent methods like Cog Attention and the Differential Transformer have addressed this by introducing negative att…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae699ba",
    "title": "Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails",
    "url": "https://arxiv.org/abs/2508.18384",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "The pervasiveness of large language models (LLMs) in enterprise settings has also brought forth a significant amount of risks associated with their usage. Guardrails technologies aim to mitigate this risk by filtering LLMs' input/output text through various detectors. However, developing and main…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae699b7",
    "title": "Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models",
    "url": "https://arxiv.org/abs/2508.18381",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Large vision-language models (LVLMs) have demonstrated exceptional capabilities in understanding visual information with human languages but also exhibit an imbalance in multilingual capabilities. In this work, we delve into the multilingual working pattern of LVLMs and identify a salient correla…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae69904",
    "title": "Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective",
    "url": "https://arxiv.org/abs/2508.18328",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "English is the predominant language on the web, powering nearly half of the world's top ten million websites. Support for multilingual content is nevertheless growing, with many websites increasingly combining English with regional or native languages in both visible content and hidden metadata. …",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae698fd",
    "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions",
    "url": "https://arxiv.org/abs/2508.18321",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Large language models (LLMs) are increasingly deployed in multi-agent systems (MAS) as components of collaborative intelligence, where peer interactions dynamically shape individual decision-making. Although prior work has focused on conformity bias, we extend the analysis to examine how LLMs for…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae69614",
    "title": "Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI",
    "url": "https://arxiv.org/abs/2508.18290",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "This essay develops a theoretical framework for a semantic Artificial General Intelligence (AGI) based on the notion of semantic attractors in complex-valued meaning spaces. Departing from current transformer-based language models, which operate on statistical next-token prediction, we explore a …",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e68970",
    "title": "Data Augmentation Improves Machine Unlearning",
    "url": "https://arxiv.org/abs/2508.18502",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Machine Unlearning (MU) aims to remove the influence of specific data from a trained model while preserving its performance on the remaining data. Although a few works suggest connections between memorisation and augmentation, the role of systematic augmentation design in MU remains under-investi…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e6868a",
    "title": "DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection",
    "url": "https://arxiv.org/abs/2508.18474",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Anomaly detection in time series data is important for applications in finance, healthcare, sensor networks, and industrial monitoring. Traditional methods usually struggle with limited labeled data, high false-positive rates, and difficulty generalizing to novel anomaly types. To overcome these …",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e68669",
    "title": "VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning",
    "url": "https://arxiv.org/abs/2508.18462",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Recent advancements in code generation have shown remarkable success across software domains, yet hardware description languages (HDLs) such as Verilog remain underexplored due to their concurrency semantics, syntactic rigidity, and simulation complexity. In this work, we address these challenges…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e685ee",
    "title": "Enhancing Trust-Region Bayesian Optimization via Newton Methods",
    "url": "https://arxiv.org/abs/2508.18423",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Bayesian Optimization (BO) has been widely applied to optimize expensive black-box functions while retaining sample efficiency. However, scaling BO to high-dimensional spaces remains challenging. Existing literature proposes performing standard BO in multiple local trust regions (TuRBO) for heter…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e685eb",
    "title": "LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning",
    "url": "https://arxiv.org/abs/2508.18420",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "This paper explores the combination of two intrinsic motivation strategies to improve the efficiency of reinforcement learning (RL) agents in environments with extreme sparse rewards, where traditional learning struggles due to infrequent positive feedback. We propose integrating Variational Stat…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e685b5",
    "title": "Low-Rank Tensor Decompositions for the Theory of Neural Networks",
    "url": "https://arxiv.org/abs/2508.18408",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "The groundbreaking performance of deep neural networks (NNs) promoted a surge of interest in providing a mathematical basis to deep learning theory. Low-rank tensor decompositions are specially befitting for this task due to their close connection to NNs and their rich theoretical results. Differ…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e682cb",
    "title": "DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction",
    "url": "https://arxiv.org/abs/2508.18376",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Mixture of Experts (MoE) has become a mainstream architecture for building Large Language Models (LLMs) by reducing per-token computation while enabling model scaling. It can be viewed as partitioning a large Feed-Forward Network (FFN) at the tensor level into fine-grained sub-FFNs, or experts, a…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e68251",
    "title": "Linear cost mutual information estimation and independence test of similar performance as HSIC",
    "url": "https://arxiv.org/abs/2508.18338",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Evaluation of statistical dependencies between two data samples is a basic problem of data science/machine learning, and HSIC (Hilbert-Schmidt Information Criterion)~\\cite{HSIC} is considered the state-of-art method. However, for size $n$ data sample it requires multiplication of $n\\times n$ matr…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e68213",
    "title": "ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation",
    "url": "https://arxiv.org/abs/2508.18318",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Wind power data often suffers from missing values due to sensor faults and unstable transmission at edge sites. While federated learning enables privacy-preserving collaboration without sharing raw data, it remains vulnerable to anomalous updates and privacy leakage during parameter exchange. The…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e68211",
    "title": "Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing",
    "url": "https://arxiv.org/abs/2508.18316",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "High dropout and failure rates in distance education pose a significant challenge for academic institutions, making the proactive identification of at-risk students crucial for providing timely support. This study develops and evaluates a machine learning model based on early academic performance…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e6820e",
    "title": "ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions",
    "url": "https://arxiv.org/abs/2508.18313",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Digital healthcare systems have enabled the collection of mass healthcare data in electronic healthcare records (EHRs), allowing artificial intelligence solutions for various healthcare prediction tasks. However, existing studies often focus on isolated components of EHR data, limiting their pred…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e6820d",
    "title": "What Matters in Data for DPO?",
    "url": "https://arxiv.org/abs/2508.18312",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Direct Preference Optimization (DPO) has emerged as a simple and effective approach for aligning large language models (LLMs) with human preferences, bypassing the need for a learned reward model. Despite its growing adoption, a fundamental question remains open: what characteristics of preferenc…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e681f4",
    "title": "CoPE: A Lightweight Complex Positional Encoding",
    "url": "https://arxiv.org/abs/2508.18308",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Recent studies have demonstrated the effectiveness of position encoding in transformer architectures. By incorporating positional information, this approach provides essential guidance for modeling dependencies between elements across different sequence positions. We introduce CoPE (a lightweight…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e681f3",
    "title": "Learning Spatio-Temporal Dynamics via Operator-Valued RKHS and Kernel Koopman Methods",
    "url": "https://arxiv.org/abs/2508.18307",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "We introduce a unified framework for learning the spatio-temporal dynamics of vector valued functions by combining operator valued reproducing kernel Hilbert spaces (OV-RKHS) with kernel based Koopman operator methods. The approach enables nonparametric and data driven estimation of complex time …",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e681f2",
    "title": "SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds",
    "url": "https://arxiv.org/abs/2508.18306",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Recent strides in pretrained transformer-based language models have propelled state-of-the-art performance in numerous NLP tasks. Yet, as these models grow in size and deployment, their robustness under input perturbations becomes an increasingly urgent question. Existing robustness methods often…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e681ef",
    "title": "Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder",
    "url": "https://arxiv.org/abs/2508.18303",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "While imaging-genetics holds great promise for unraveling the complex interplay between brain structure and genetic variation in neurological disorders, traditional methods are limited to simplistic linear models or to black-box techniques that lack interpretability. In this paper, we present Neu…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e681ed",
    "title": "A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach",
    "url": "https://arxiv.org/abs/2508.18301",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Background: Existing robust, pervasive device-based systems developed in recent years to detect depression require data collected over a long period and may not be effective in cases where early detection is crucial. Objective: Our main objective was to develop a minimalistic system to identify d…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e67f2c",
    "title": "Data-driven models for production forecasting and decision supporting in petroleum reservoirs",
    "url": "https://arxiv.org/abs/2508.18289",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Forecasting production reliably and anticipating changes in the behavior of rock-fluid systems are the main challenges in petroleum reservoir engineering. This project proposes to deal with this problem through a data-driven approach and using machine learning methods. The objective is to develop…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e67f27",
    "title": "Multi-Modal Drift Forecasting of Leeway Objects via Navier-Stokes-Guided CNN and Sequence-to-Sequence Attention-Based Models",
    "url": "https://arxiv.org/abs/2508.18284",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Accurately predicting the drift (displacement) of leeway objects in maritime environments remains a critical challenge, particularly in time-sensitive scenarios such as search and rescue operations. In this study, we propose a multi-modal machine learning framework that integrates Sentence Transf…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e67f0d",
    "title": "Reasoning Steps as Curriculum: Using Depth of Thought as a Difficulty Signal for Tuning LLMs",
    "url": "https://arxiv.org/abs/2508.18279",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Curriculum learning for training LLMs requires a difficulty signal that aligns with reasoning while remaining scalable and interpretable. We propose a simple premise: tasks that demand deeper depth of thought for humans should also be harder for models. Accordingly, we define difficulty as depth …",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_55d5dc7f",
    "title": "Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction",
    "url": "https://arxiv.org/abs/2508.18751",
    "source": "arXiv cs.AI",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Deep neural networks demonstrate strong performance under aligned training-test distributions. However, real-world test data often exhibit domain shifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the model to test data during inference. While most TTA studies assume that the…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_55d5dc68",
    "title": "Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution",
    "url": "https://arxiv.org/abs/2508.18749",
    "source": "arXiv cs.AI",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Recent advances in prompt optimization, exemplified by methods such as TextGrad, enable automatic, gradient-like refinement of textual prompts to enhance the performance of large language models (LLMs) on specific downstream tasks. However, current approaches are typically stateless and operate i…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_55d5dc62",
    "title": "CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks",
    "url": "https://arxiv.org/abs/2508.18743",
    "source": "arXiv cs.AI",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs) solve difficult problems, but very long traces often slow or even degrade performance on fast, intuitive \"System-1\" tasks. We introduce Connector-Aware Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning …",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_55d5dc25",
    "title": "Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval",
    "url": "https://arxiv.org/abs/2508.18724",
    "source": "arXiv cs.AI",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Large Language Models (LLMs) have transformed the field of artificial intelligence by unlocking the era of generative applications. Built on top of generative AI capabilities, Agentic AI represents a major shift toward autonomous, goal-driven systems that can reason, retrieve, and act. However, t…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_55d5dc23",
    "title": "VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft",
    "url": "https://arxiv.org/abs/2508.18722",
    "source": "arXiv cs.AI",
    "publishedAt": "2025-08-27",
    "tags": [
      "rss_feed"
    ],
    "summary": "Large language models (LLMs) have shown significant promise in embodied decision-making tasks within virtual open-world environments. Nonetheless, their performance is hindered by the absence of domain-specific knowledge. Methods that finetune on large-scale domain-specific data entail prohibitiv…",
    "sourceDomain": "arxiv.org"
  }
]