[
  {
    "id": "rss_e4d9c8ea",
    "title": "Google’s URL Context Grounding: Another Nail in RAG’s Coffin? | Towards Data Science",
    "url": "https://towardsdatascience.com/googles-url-context-grounding-another-nail-in-rags-coffin/",
    "source": "Towards Data Science",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Google’s hot streak in AI-related releases continues unabated. Just a few days ago, it released a new tool for Gemini called URL context grounding. URL context grounding can be used stand-alone or combined with Google search grounding to conduct deep dives into internet content. What is URL conte…",
    "sourceDomain": "towardsdatascience.com"
  },
  {
    "id": "rss_adb47098",
    "title": "Positional Embeddings in Transformers: A Math Guide to RoPE & ALiBi | Towards Data Science",
    "url": "https://towardsdatascience.com/positional-embeddings-in-transformers-a-math-guide-to-rope-alibi/",
    "source": "Towards Data Science",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Learn APE, RoPE, and ALiBi positional embeddings for GPT — intuitions, math, PyTorch code, and experiments on TinyStories",
    "sourceDomain": "towardsdatascience.com"
  },
  {
    "id": "rss_b30bc965",
    "title": "Using Google’s LangExtract and Gemma for Structured Data Extraction | Towards Data Science",
    "url": "https://towardsdatascience.com/using-googles-langextract-and-gemma-for-structured-data-extraction/",
    "source": "Towards Data Science",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Extracting structured information effectively and accurately from long unstructured text with LangExtract and LLMs",
    "sourceDomain": "towardsdatascience.com"
  },
  {
    "id": "rss_4285dc63",
    "title": "Plato’s Cave and the Shadows of Data | Towards Data Science",
    "url": "https://towardsdatascience.com/platos-cave-and-the-shadows-of-data/",
    "source": "Towards Data Science",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "On truth, illusion, and the limits of what data can reveal",
    "sourceDomain": "towardsdatascience.com"
  },
  {
    "id": "rss_23e14bea",
    "title": "How to Develop Powerful Internal LLM Benchmarks | Towards Data Science",
    "url": "https://towardsdatascience.com/how-to-develop-powerf-interal-llm-benchmarks/",
    "source": "Towards Data Science",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Learn how to compare LLMs using your own interal benchmark",
    "sourceDomain": "towardsdatascience.com"
  },
  {
    "id": "rss_3c92788e",
    "title": "Crescent library brings privacy to digital identity systems",
    "url": "https://www.microsoft.com/en-us/research/blog/crescent-library-brings-privacy-to-digital-identity-systems/",
    "source": "Microsoft Research Blog",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Crescent helps make digital IDs private by preventing tracking across uses while letting users only disclose what’s necessary from their credentials:",
    "sourceDomain": "microsoft.com"
  },
  {
    "id": "rss_bb859027",
    "title": "Image editing in Gemini just got a major upgrade",
    "url": "https://deepmind.google/discover/blog/image-editing-in-gemini-just-got-a-major-upgrade/",
    "source": "DeepMind Blog",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Transform images in amazing new ways with updated native image editing in the Gemini app.",
    "sourceDomain": "deepmind.google"
  },
  {
    "id": "rss_dae5c3c6",
    "title": "ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks",
    "url": "https://arxiv.org/abs/2508.16889",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Large language models (LLMs) are increasingly used as judges of other models, yet it is unclear whether a judge can reliably infer the latent objective of the conversation it evaluates, especially when the goal is distributed across noisy, adversarial, multi-turn jailbreaks. We introduce OBJEX(MT…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5c3a4",
    "title": "Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling",
    "url": "https://arxiv.org/abs/2508.16876",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "World models have been widely utilized in robotics, gaming, and auto-driving. However, their applications on natural language tasks are relatively limited. In this paper, we construct the dialogue world model, which could predict the user's emotion, sentiment, and intention, and future utterances…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5c39e",
    "title": "JUDGEBERT: Assessing Legal Meaning Preservation Between Sentences",
    "url": "https://arxiv.org/abs/2508.16870",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Simplifying text while preserving its meaning is a complex yet essential task, especially in sensitive domain applications like legal texts. When applied to a specialized field, like the legal domain, preservation differs significantly from its role in regular texts. This paper introduces FrJUDGE…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5c386",
    "title": "QFrCoLA: a Quebec-French Corpus of Linguistic Acceptability Judgments",
    "url": "https://arxiv.org/abs/2508.16867",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Large and Transformer-based language models perform outstandingly in various downstream tasks. However, there is limited understanding regarding how these models internalize linguistic knowledge, so various linguistic benchmarks have recently been proposed to facilitate syntactic evaluation of la…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5c380",
    "title": "Learning from Diverse Reasoning Paths with Routing and Collaboration",
    "url": "https://arxiv.org/abs/2508.16861",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Advances in large language models (LLMs) significantly enhance reasoning capabilities but their deployment is restricted in resource-constrained scenarios. Knowledge distillation addresses this by transferring knowledge from powerful teacher models to compact and transparent students. However, ef…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5c32a",
    "title": "If We May De-Presuppose: Robustly Verifying Claims through Presupposition-Free Question Decomposition",
    "url": "https://arxiv.org/abs/2508.16838",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Prior work has shown that presupposition in generated questions can introduce unverified assumptions, leading to inconsistencies in claim verification. Additionally, prompt sensitivity remains a significant challenge for large language models (LLMs), resulting in performance variance as high as 3…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5c329",
    "title": "LLMs Learn Constructions That Humans Do Not Know",
    "url": "https://arxiv.org/abs/2508.16837",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "This paper investigates false positive constructions: grammatical structures which an LLM hallucinates as distinct constructions but which human introspection does not support. Both a behavioural probing task using contextual embeddings and a meta-linguistic probing task using prompts are include…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5c325",
    "title": "ReProCon: Scalable and Resource-Efficient Few-Shot Biomedical Named Entity Recognition",
    "url": "https://arxiv.org/abs/2508.16833",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Named Entity Recognition (NER) in biomedical domains faces challenges due to data scarcity and imbalanced label distributions, especially with fine-grained entity types. We propose ReProCon, a novel few-shot NER framework that combines multi-prototype modeling, cosine-contrastive learning, and Re…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5c004",
    "title": "Assess and Prompt: A Generative RL Framework for Improving Engagement in Online Mental Health Communities",
    "url": "https://arxiv.org/abs/2508.16788",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Online Mental Health Communities (OMHCs) provide crucial peer and expert support, yet many posts remain unanswered due to missing support attributes that signal the need for help. We present a novel framework that identifies these gaps and prompts users to enrich their posts, thereby improving en…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5bfc0",
    "title": "Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence Through Multimodal Story Generation",
    "url": "https://arxiv.org/abs/2508.16762",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "As Vision-Language Models (VLMs) achieve widespread deployment across diverse cultural contexts, ensuring their cultural competence becomes critical for responsible AI systems. While prior work has evaluated cultural awareness in text-only models and VLM object recognition tasks, no research has …",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5bfa6",
    "title": "How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models",
    "url": "https://arxiv.org/abs/2508.16757",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "In this work, we present a systematic and comprehensive empirical evaluation of state-of-the-art reranking methods, encompassing large language model (LLM)-based, lightweight contextual, and zero-shot approaches, with respect to their performance in information retrieval tasks. We evaluate in tot…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5bfa2",
    "title": "GAICo: A Deployed and Extensible Framework for Evaluating Diverse and Multimodal Generative AI Outputs",
    "url": "https://arxiv.org/abs/2508.16753",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "The rapid proliferation of Generative AI (GenAI) into diverse, high-stakes domains necessitates robust and reproducible evaluation methods. However, practitioners often resort to ad-hoc, non-standardized scripts, as common metrics are often unsuitable for specialized, structured outputs (e.g., au…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5bf4b",
    "title": "Error Reflection Prompting: Can Large Language Models Successfully Understand Errors?",
    "url": "https://arxiv.org/abs/2508.16729",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Prompting methods for language models, such as Chain-of-thought (CoT), present intuitive step-by-step processes for problem solving. These methodologies aim to equip models with a better understanding of the correct procedures for addressing a given task. Despite these advancements, CoT lacks the…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5bf0b",
    "title": "Sparse and Dense Retrievers Learn Better Together: Joint Sparse-Dense Optimization for Text-Image Retrieval",
    "url": "https://arxiv.org/abs/2508.16707",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Vision-Language Pretrained (VLP) models have achieved impressive performance on multimodal tasks, including text-image retrieval, based on dense representations. Meanwhile, Learned Sparse Retrieval (LSR) has gained traction in text-only settings due to its interpretability and efficiency with fas…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5bf09",
    "title": "Assessing Consciousness-Related Behaviors in Large Language Models Using the Maze Test",
    "url": "https://arxiv.org/abs/2508.16705",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "We investigate consciousness-like behaviors in Large Language Models (LLMs) using the Maze Test, challenging models to navigate mazes from a first-person perspective. This test simultaneously probes spatial awareness, perspective-taking, goal-directed behavior, and temporal sequencing-key conscio…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5bc61",
    "title": "QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting",
    "url": "https://arxiv.org/abs/2508.16697",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Advanced reasoning capabilities in Large Language Models (LLMs) have caused higher hallucination prevalence; yet most mitigation work focuses on after-the-fact filtering rather than shaping the queries that trigger them. We introduce QueryBandits, a bandit framework that designs rewrite strategie…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5bc5f",
    "title": "Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?",
    "url": "https://arxiv.org/abs/2508.16695",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Recent progress in reasoning-oriented Large Language Models (LLMs) has been driven by introducing Chain-of-Thought (CoT) traces, where models generate intermediate reasoning traces before producing an answer. These traces, as in DeepSeek R1, are not only used to guide inference but also serve as …",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5bc02",
    "title": "Trust but Verify! A Survey on Verification Design for Test-time Scaling",
    "url": "https://arxiv.org/abs/2508.16665",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Test-time scaling (TTS) has emerged as a new frontier for scaling the performance of Large Language Models. In test-time scaling, by using more computational resources during inference, LLMs can improve their reasoning process and task performance. Several approaches have emerged for TTS such as …",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5bba6",
    "title": "Cognitive Decision Routing in Large Language Models: When to Think Fast, When to Think Slow",
    "url": "https://arxiv.org/abs/2508.16636",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Large Language Models (LLMs) face a fundamental challenge in deciding when to rely on rapid, intuitive responses versus engaging in slower, more deliberate reasoning. Inspired by Daniel Kahneman's dual-process theory and his insights on human cognitive biases, we propose a novel Cognitive Decisio…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_dae5bb46",
    "title": "GreenTEA: Gradient Descent with Topic-modeling and Evolutionary Auto-prompting",
    "url": "https://arxiv.org/abs/2508.16603",
    "source": "arXiv cs.CL",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "High-quality prompts are crucial for Large Language Models (LLMs) to achieve exceptional performance. However, manually crafting effective prompts is labor-intensive and demands significant domain expertise, limiting its scalability. Existing automatic prompt optimization methods either extensive…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a569",
    "title": "CALR: Corrective Adaptive Low-Rank Decomposition for Efficient Large Language Model Layer Compression",
    "url": "https://arxiv.org/abs/2508.16680",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Large Language Models (LLMs) present significant deployment challenges due to their immense size and computational requirements. Model compression techniques are essential for making these models practical for resource-constrained environments. A prominent compression strategy is low-rank factori…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a551",
    "title": "Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration",
    "url": "https://arxiv.org/abs/2508.16677",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Many existing studies have achieved significant improvements in the reasoning capabilities of large language models (LLMs) through reinforcement learning with verifiable rewards (RLVR), while the enhancement of reasoning abilities in small language models (SLMs) has not yet been sufficiently expl…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a550",
    "title": "WISCA: A Lightweight Model Transition Method to Improve LLM Training via Weight Scaling",
    "url": "https://arxiv.org/abs/2508.16676",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Transformer architecture gradually dominates the LLM field. Recent advances in training optimization for Transformer-based large language models (LLMs) primarily focus on architectural modifications or optimizer adjustments. However, these approaches lack systematic optimization of weight pattern…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a512",
    "title": "OASIS: Open-world Adaptive Self-supervised and Imbalanced-aware System",
    "url": "https://arxiv.org/abs/2508.16656",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "The expansion of machine learning into dynamic environments presents challenges in handling open-world problems where label shift, covariate shift, and unknown classes emerge. Post-training methods have been explored to address these challenges, adapting models to newly emerging data. However, th…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a511",
    "title": "A Laplace diffusion-based transformer model for heart rate forecasting within daily activity context",
    "url": "https://arxiv.org/abs/2508.16655",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "With the advent of wearable Internet of Things (IoT) devices, remote patient monitoring (RPM) emerged as a promising solution for managing heart failure. However, the heart rate can fluctuate significantly due to various factors, and without correlating it to the patient's actual physical activit…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a50d",
    "title": "HiCL: Hippocampal-Inspired Continual Learning",
    "url": "https://arxiv.org/abs/2508.16651",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "We propose HiCL, a novel hippocampal-inspired dual-memory continual learning architecture designed to mitigate catastrophic forgetting by using elements inspired by the hippocampal circuitry. Our system encodes inputs through a grid-cell-like layer, followed by sparse pattern separation using a d…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a4f5",
    "title": "LatentFlow: Cross-Frequency Experimental Flow Reconstruction from Sparse Pressure via Latent Mapping",
    "url": "https://arxiv.org/abs/2508.16648",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Acquiring temporally high-frequency and spatially high-resolution turbulent wake flow fields in particle image velocimetry (PIV) experiments remains a significant challenge due to hardware limitations and measurement noise. In contrast, temporal high-frequency measurements of spatially sparse wal…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a4f4",
    "title": "AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling for Edge DNN Training",
    "url": "https://arxiv.org/abs/2508.16647",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Training deep neural networks (DNNs) directly on edge devices has attracted increasing attention, as it offers promising solutions to challenges such as domain adaptation and privacy preservation. However, conventional DNN training typically requires large-scale datasets, which imposes prohibitiv…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a4f0",
    "title": "From Classical Probabilistic Latent Variable Models to Modern Generative AI: A Unified Perspective",
    "url": "https://arxiv.org/abs/2508.16643",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "From large language models to multi-modal agents, Generative Artificial Intelligence (AI) now underpins state-of-the-art systems. Despite their varied architectures, many share a common foundation in probabilistic latent variable models (PLVMs), where hidden variables explain observed data for de…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a4ee",
    "title": "Enhancing Transformer-Based Foundation Models for Time Series Forecasting via Bagging, Boosting and Statistical Ensembles",
    "url": "https://arxiv.org/abs/2508.16641",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Time series foundation models (TSFMs) such as Lag-Llama, TimeGPT, Chronos, MOMENT, UniTS, and TimesFM have shown strong generalization and zero-shot capabilities for time series forecasting, anomaly detection, classification, and imputation. Despite these advantages, their predictions still suffe…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a4d2",
    "title": "Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations",
    "url": "https://arxiv.org/abs/2508.16634",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to continuously learn from new fault classes with only a few samples without forgetting old ones, is critical for real-world industrial systems. However, this challenging task severely amplifies the issues of catastrophic forgetting …",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a4d1",
    "title": "A Novel Unified Extended Matrix for Graph Signal Processing: Theory and Application",
    "url": "https://arxiv.org/abs/2508.16633",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Graph signal processing has become an essential tool for analyzing data structured on irregular domains. While conventional graph shift operators (GSOs) are effective for certain tasks, they inherently lack flexibility in modeling dependencies between non-adjacent nodes, limiting their ability to…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a4d0",
    "title": "Adaptive Variance-Penalized Continual Learning with Fisher Regularization",
    "url": "https://arxiv.org/abs/2508.16632",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "The persistent challenge of catastrophic forgetting in neural networks has motivated extensive research in continual learning . This work presents a novel continual learning framework that integrates Fisher-weighted asymmetric regularization of parameter variances within a variational learning pa…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a4cf",
    "title": "Recurrent Transformer U-Net Surrogate for Flow Modeling and Data Assimilation in Subsurface Formations with Faults",
    "url": "https://arxiv.org/abs/2508.16631",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Many subsurface formations, including some of those under consideration for large-scale geological carbon storage, include extensive faults that can strongly impact fluid flow. In this study, we develop a new recurrent transformer U-Net surrogate model to provide very fast predictions for pressur…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a4b8",
    "title": "Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework",
    "url": "https://arxiv.org/abs/2508.16629",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "LLM-based agents have been extensively applied across various domains, where memory stands out as one of their most essential capabilities. Previous memory mechanisms of LLM-based agents are manually predefined by human experts, leading to higher labor costs and suboptimal performance. In additio…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a4b2",
    "title": "A Retrieval Augmented Spatio-Temporal Framework for Traffic Prediction",
    "url": "https://arxiv.org/abs/2508.16623",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Traffic prediction is a cornerstone of modern intelligent transportation systems and a critical task in spatio-temporal forecasting. Although advanced Spatio-temporal Graph Neural Networks (STGNNs) and pre-trained models have achieved significant progress in traffic prediction, two key challenges…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a4af",
    "title": "STRelay: A Universal Spatio-Temporal Relaying Framework for Location Prediction with Future Spatiotemporal Contexts",
    "url": "https://arxiv.org/abs/2508.16620",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Next location prediction is a critical task in human mobility modeling, enabling applications like travel planning and urban mobility management. Existing methods mainly rely on historical spatiotemporal trajectory data to train sequence models that directly forecast future locations. However, th…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a497",
    "title": "Leveraging the Christoffel Function for Outlier Detection in Data Streams",
    "url": "https://arxiv.org/abs/2508.16617",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Outlier detection holds significant importance in the realm of data mining, particularly with the growing pervasiveness of data acquisition methods. The ability to identify outliers in data streams is essential for maintaining data quality and detecting faults. However, dealing with data streams …",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a494",
    "title": "CrystalDiT: A Diffusion Transformer for Crystal Generation",
    "url": "https://arxiv.org/abs/2508.16614",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "We present CrystalDiT, a diffusion transformer for crystal structure generation that achieves state-of-the-art performance by challenging the trend of architectural complexity. Instead of intricate, multi-stream designs, CrystalDiT employs a unified transformer that imposes a powerful inductive b…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_46e5a491",
    "title": "Quantum-Inspired DRL Approach with LSTM and OU Noise for Cut Order Planning Optimization",
    "url": "https://arxiv.org/abs/2508.16611",
    "source": "arXiv cs.LG",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Cut order planning (COP) is a critical challenge in the textile industry, directly impacting fabric utilization and production costs. Conventional methods based on static heuristics and catalog-based estimations often struggle to adapt to dynamic production environments, resulting in suboptimal s…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_55d5553f",
    "title": "L-XAIDS: A LIME-based eXplainable AI framework for Intrusion Detection Systems",
    "url": "https://arxiv.org/abs/2508.17244",
    "source": "arXiv cs.AI",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Recent developments in Artificial Intelligence (AI) and their applications in critical industries such as healthcare, fin-tech and cybersecurity have led to a surge in research in explainability in AI. Innovative research methods are being explored to extract meaningful insight from blackbox AI s…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_55d554fe",
    "title": "MC3G: Model Agnostic Causally Constrained Counterfactual Generation",
    "url": "https://arxiv.org/abs/2508.17221",
    "source": "arXiv cs.AI",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Machine learning models increasingly influence decisions in high-stakes settings such as finance, law and hiring, driving the need for transparent, interpretable outcomes. However, while explainable approaches can help understand the decisions being made, they may inadvertently reveal the underly…",
    "sourceDomain": "arxiv.org"
  },
  {
    "id": "rss_55d554e0",
    "title": "Reinforcement Learning enhanced Online Adaptive Clinical Decision Support via Digital Twin powered Policy and Treatment Effect optimized Reward",
    "url": "https://arxiv.org/abs/2508.17212",
    "source": "arXiv cs.AI",
    "publishedAt": "2025-08-26",
    "tags": [
      "rss_feed"
    ],
    "summary": "Clinical decision support must adapt online under safety constraints. We present an online adaptive tool where reinforcement learning provides the policy, a patient digital twin provides the environment, and treatment effect defines the reward. The system initializes a batch-constrained policy fr…",
    "sourceDomain": "arxiv.org"
  }
]