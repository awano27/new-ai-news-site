#!/usr/bin/env python3
"""æ‹¡å¼µã•ã‚ŒãŸAIãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆXã®ä»£æ›¿å«ã‚€ï¼‰"""

import sys
import asyncio
import requests
import feedparser
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any
import time
import re
import json

# Add src to path
project_root = Path(__file__).parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

from src.config.settings import Settings
from src.models.article import Article, TechnicalMetadata, BusinessMetadata, ImplementationCost
from src.generators.static_site_generator import StaticSiteGenerator

class EnhancedAINewsCollector:
    """æ‹¡å¼µAIãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆXã®ä»£æ›¿ã‚½ãƒ¼ã‚¹å«ã‚€ï¼‰"""
    
    def __init__(self):
        self.sources = {
            # ===== Tier 1: æœ€é«˜å“è³ªã‚½ãƒ¼ã‚¹ =====
            
            # è‹±èªåœä¸»è¦ã‚½ãƒ¼ã‚¹
            "openai_blog": {
                "url": "https://openai.com/blog/rss.xml",
                "tier": 1,
                "source_name": "OpenAI Blog",
                "category": "ai_company",
                "lang": "en"
            },
            "anthropic_news": {
                "url": "https://anthropic.com/news/rss",
                "backup_url": "https://anthropic.com/news",
                "tier": 1,
                "source_name": "Anthropic News",
                "category": "ai_company",
                "lang": "en"
            },
            "deepmind_blog": {
                "url": "https://deepmind.google/discover/blog/rss.xml",
                "tier": 1,
                "source_name": "DeepMind Blog",
                "category": "ai_research",
                "lang": "en"
            },
            "google_ai_blog": {
                "url": "https://blog.research.google/feeds/posts/default",
                "tier": 1,
                "source_name": "Google Research Blog",
                "category": "ai_research",
                "lang": "en"
            },
            "huggingface_blog": {
                "url": "https://huggingface.co/blog/rss.xml",
                "tier": 1,
                "source_name": "Hugging Face Blog",
                "category": "ai_platform",
                "lang": "en"
            },
            
            # ç ”ç©¶æ©Ÿé–¢
            "mit_csail": {
                "url": "https://www.csail.mit.edu/rss.xml",
                "tier": 1,
                "source_name": "MIT CSAIL",
                "category": "ai_research",
                "lang": "en"
            },
            "stanford_ai": {
                "url": "https://hai.stanford.edu/feed",
                "tier": 1,
                "source_name": "Stanford HAI",
                "category": "ai_research", 
                "lang": "en"
            },
            
            # ãƒ“ã‚¸ãƒã‚¹ãƒ»ãƒ†ãƒƒã‚¯ãƒ¡ãƒ‡ã‚£ã‚¢ï¼ˆè‹±èªï¼‰
            "mit_tech_review": {
                "url": "https://www.technologyreview.com/topic/artificial-intelligence/feed/",
                "tier": 1,
                "source_name": "MIT Technology Review",
                "category": "tech_media",
                "lang": "en"
            },
            "venturebeat_ai": {
                "url": "https://venturebeat.com/ai/feed/",
                "tier": 1,
                "source_name": "VentureBeat AI",
                "category": "tech_media",
                "lang": "en"
            },
            "techcrunch_ai": {
                "url": "https://techcrunch.com/category/artificial-intelligence/feed/",
                "tier": 1,
                "source_name": "TechCrunch AI",
                "category": "tech_media",
                "lang": "en"
            },
            "the_verge_ai": {
                "url": "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml",
                "tier": 1,
                "source_name": "The Verge AI",
                "category": "tech_media",
                "lang": "en"
            },
            
            # ===== æ—¥æœ¬èªã‚½ãƒ¼ã‚¹ =====
            "itmedia_ai": {
                "url": "https://rss.itmedia.co.jp/rss/2.0/ait.xml",
                "tier": 1,
                "source_name": "ITmedia AI",
                "category": "tech_media_jp",
                "lang": "ja"
            },
            "itmedia_news_tech": {
                "url": "https://rss.itmedia.co.jp/rss/2.0/news_technology.xml",
                "tier": 1,
                "source_name": "ITmedia ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼",
                "category": "tech_media_jp",
                "lang": "ja"
            },
            "internet_watch": {
                "url": "https://internet.watch.impress.co.jp/data/rss/1.0/iw/feed.rdf",
                "tier": 1,
                "source_name": "INTERNET Watch",
                "category": "tech_media_jp",
                "lang": "ja"
            },
            "pc_watch": {
                "url": "https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf",
                "tier": 1,
                "source_name": "PC Watch",
                "category": "tech_media_jp",
                "lang": "ja"
            },
            "gizmodo_jp": {
                "url": "https://www.gizmodo.jp/index.xml",
                "tier": 1,
                "source_name": "Gizmodo Japan",
                "category": "tech_media_jp",
                "lang": "ja"
            },
            "cnet_japan": {
                "url": "https://feeds.japan.cnet.com/rss/cnet/all.rdf",
                "tier": 1,
                "source_name": "CNET Japan",
                "category": "tech_media_jp",
                "lang": "ja"
            },
            
            # ===== Tier 2: é‡è¦ã‚½ãƒ¼ã‚¹ =====
            
            # é–‹ç™ºè€…ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
            "hackernews_ai": {
                "url": "https://hnrss.org/newest?q=AI+OR+artificial+intelligence+OR+machine+learning",
                "tier": 2,
                "source_name": "Hacker News AI",
                "category": "community",
                "lang": "en"
            },
            "reddit_ml": {
                "url": "https://www.reddit.com/r/MachineLearning/.rss",
                "tier": 2,
                "source_name": "Reddit MachineLearning",
                "category": "community",
                "lang": "en"
            },
            "towards_data_science": {
                "url": "https://towardsdatascience.com/feed/tagged/artificial-intelligence",
                "tier": 2,
                "source_name": "Towards Data Science",
                "category": "tutorial",
                "lang": "en"
            },
            
            # è¿½åŠ ã®ç ”ç©¶ã‚½ãƒ¼ã‚¹
            "arxiv_ai": {
                "url": "http://export.arxiv.org/rss/cs.AI/recent",
                "tier": 2,
                "source_name": "arXiv AI",
                "category": "research",
                "lang": "en"
            },
            "arxiv_ml": {
                "url": "http://export.arxiv.org/rss/cs.LG/recent",
                "tier": 2,
                "source_name": "arXiv Machine Learning", 
                "category": "research",
                "lang": "en"
            },
            
            # ===== Xï¼ˆTwitterï¼‰ã®ä»£æ›¿ã¨ã—ã¦ã€AIé–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚°ãƒªã‚²ãƒ¼ã‚¿ =====
            
            # AIãƒ‹ãƒ¥ãƒ¼ã‚¹å°‚é–€ã‚µã‚¤ãƒˆ
            "ai_news": {
                "url": "https://artificialintelligence-news.com/feed/",
                "tier": 2,
                "source_name": "AI News",
                "category": "ai_news",
                "lang": "en"
            },
            "machine_learning_mastery": {
                "url": "https://machinelearningmastery.com/feed/",
                "tier": 2,
                "source_name": "Machine Learning Mastery",
                "category": "tutorial",
                "lang": "en"
            },
            
            # å¤šè¨€èªå¯¾å¿œã®ãƒ†ãƒƒã‚¯ç³»
            "wired_ai": {
                "url": "https://www.wired.com/tag/artificial-intelligence/feed/",
                "tier": 2,
                "source_name": "WIRED AI",
                "category": "tech_media",
                "lang": "en"
            },
            "ars_technica": {
                "url": "https://arstechnica.com/tag/artificial-intelligence/feed/",
                "tier": 2,
                "source_name": "Ars Technica AI",
                "category": "tech_media",
                "lang": "en"
            }
        }
    
    def enhance_ai_filtering(self, content: str, title: str) -> bool:
        """å¼·åŒ–ã•ã‚ŒãŸAIé–¢é€£ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        text = (title + " " + content).lower()
        
        # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰
        ai_keywords = [
            # è‹±èª
            'artificial intelligence', 'ai', 'machine learning', 'ml', 'deep learning',
            'neural network', 'chatgpt', 'gpt', 'llm', 'large language model',
            'transformer', 'bert', 'nlp', 'computer vision', 'generative ai',
            'langchain', 'openai', 'anthropic', 'claude', 'gemini', 'bard',
            'hugging face', 'pytorch', 'tensorflow', 'scikit-learn',
            
            # æ—¥æœ¬èª
            'äººå·¥çŸ¥èƒ½', 'AI', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'æ·±å±¤å­¦ç¿’',
            'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯', 'è‡ªç„¶è¨€èªå‡¦ç†', 'ãƒãƒ£ãƒƒãƒˆGPT', 'ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ',
            'LLM', 'å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«', 'AIæŠ€è¡“', 'AIæ´»ç”¨', 'AIå°å…¥',
            'ç”ŸæˆAI', 'å¯¾è©±AI', 'ç”»åƒç”Ÿæˆ', 'ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ'
        ]
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆAIã¨ç„¡é–¢ä¿‚ï¼‰
        exclude_keywords = [
            'air conditioning', 'artificial insemination', 'adobe illustrator',
            'american idol', 'athletic identity', 'artificial intelligence movie'
        ]
        
        # é™¤å¤–ãƒã‚§ãƒƒã‚¯
        if any(exclude.lower() in text for exclude in exclude_keywords):
            return False
        
        # AIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        return any(keyword.lower() in text for keyword in ai_keywords)
    
    def collect_from_feed(self, feed_config: Dict, max_articles: int = 3) -> List[Article]:
        """æ‹¡å¼µãƒ•ã‚£ãƒ¼ãƒ‰åé›†"""
        articles = []
        
        try:
            print(f"ğŸ“¡ åé›†ä¸­: {feed_config['source_name']} ({feed_config['lang']})...")
            
            headers = {
                'User-Agent': 'DailyAINews/2.0 (Educational AI Research Project)',
                'Accept': 'application/rss+xml, application/xml, text/xml, application/atom+xml',
                'Accept-Language': 'ja,en;q=0.9'
            }
            
            response = requests.get(feed_config['url'], headers=headers, timeout=20)
            
            if response.status_code != 200:
                print(f"  âš ï¸ HTTP {response.status_code}")
                return articles
            
            # RSSè§£æ
            feed = feedparser.parse(response.content)
            
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                print(f"  âš ï¸ ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãªã—")
                return articles
            
            print(f"  ğŸ“„ {len(feed.entries)}å€‹ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’ç™ºè¦‹")
            
            for entry in feed.entries[:max_articles * 2]:  # ã‚ˆã‚Šå¤šãå–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿
                try:
                    # æ—¥ä»˜å‡¦ç†
                    pub_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        pub_date = datetime(*entry.updated_parsed[:6])
                    else:
                        pub_date = datetime.now()
                    
                    # 7æ—¥ä»¥å†…ã®è¨˜äº‹ã®ã¿ï¼ˆXã®ä»£æ›¿ã¨ã—ã¦æ–°é®®ã•ã‚’é‡è¦–ï¼‰
                    if pub_date < datetime.now() - timedelta(days=7):
                        continue
                    
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
                    content = ""
                    title = entry.title if hasattr(entry, 'title') else ""
                    
                    if hasattr(entry, 'summary'):
                        content = entry.summary
                    elif hasattr(entry, 'description'):
                        content = entry.description
                    elif hasattr(entry, 'content'):
                        if isinstance(entry.content, list) and len(entry.content) > 0:
                            content = entry.content[0].get('value', '')
                    
                    # HTMLã‚¿ã‚°å‰Šé™¤
                    content = re.sub(r'<[^>]+>', '', content)
                    content = content.strip()
                    
                    # æœ€ä½æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
                    if len(content) < 30:
                        continue
                    
                    # å¼·åŒ–ã•ã‚ŒãŸAIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if not self.enhance_ai_filtering(content, title):
                        continue
                    
                    # è¨˜äº‹ä½œæˆ
                    article = Article(
                        id=f"{feed_config['category']}_{feed_config['source_name'].lower().replace(' ', '_')}_{hash(entry.link) % 100000}",
                        title=title,
                        url=entry.link,
                        source=feed_config['source_name'],
                        source_tier=feed_config['tier'],
                        published_date=pub_date,
                        content=content[:500] + "..." if len(content) > 500 else content,
                        tags=[feed_config['category'], feed_config['lang'], 'ai']
                    )
                    
                    articles.append(article)
                    print(f"    âœ… åé›†: {title[:50]}...")
                    
                    # ç›®æ¨™æ•°ã«é”ã—ãŸã‚‰åœæ­¢
                    if len(articles) >= max_articles:
                        break
                    
                except Exception as e:
                    print(f"    âš ï¸ ã‚¨ãƒ³ãƒˆãƒªãƒ¼å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)[:60]}")
                    continue
            
            print(f"  âœ… {feed_config['source_name']}: {len(articles)}è¨˜äº‹åé›†å®Œäº†")
            
        except Exception as e:
            print(f"  âŒ {feed_config['source_name']}åé›†ã‚¨ãƒ©ãƒ¼: {str(e)[:80]}")
        
        return articles
    
    def collect_all_sources(self) -> List[Article]:
        """å…¨æ‹¡å¼µã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†"""
        all_articles = []
        
        print("ğŸŒ æ‹¡å¼µAIãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹ï¼ˆXã®ä»£æ›¿å«ã‚€ï¼‰")
        print("=" * 60)
        
        # ã‚½ãƒ¼ã‚¹ã‚’å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_sources = sorted(self.sources.items(), key=lambda x: x[1]['tier'])
        
        for source_key, config in sorted_sources:
            articles = self.collect_from_feed(config)
            all_articles.extend(articles)
            time.sleep(1)  # å„ã‚½ãƒ¼ã‚¹ã®é–“ã«çŸ­ã„ä¼‘æ†©
        
        # é«˜åº¦ãªé‡è¤‡å‰Šé™¤
        unique_articles = self.deduplicate_articles(all_articles)
        
        print(f"\nğŸ“Š æ‹¡å¼µåé›†çµæœ:")
        print(f"  â€¢ ç·è¨˜äº‹æ•°: {len(all_articles)}")
        print(f"  â€¢ é‡è¤‡é™¤å»å¾Œ: {len(unique_articles)}")
        print(f"  â€¢ æ—¥æœ¬èªè¨˜äº‹: {len([a for a in unique_articles if 'ja' in a.tags])}")
        print(f"  â€¢ è‹±èªè¨˜äº‹: {len([a for a in unique_articles if 'en' in a.tags])}")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        categories = {}
        for article in unique_articles:
            for tag in article.tags:
                if tag not in ['ai', 'ja', 'en']:
                    categories[tag] = categories.get(tag, 0) + 1
        
        print(f"\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥:")
        for category, count in sorted(categories.items()):
            print(f"  â€¢ {category}: {count}è¨˜äº‹")
        
        return unique_articles
    
    def deduplicate_articles(self, articles: List[Article]) -> List[Article]:
        """é«˜åº¦ãªé‡è¤‡å‰Šé™¤"""
        unique_articles = []
        seen_urls = set()
        seen_titles = set()
        
        for article in articles:
            # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯
            if article.url in seen_urls:
                continue
            
            # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
            title_key = re.sub(r'[^\w\s]', '', article.title.lower()).replace(' ', '')[:50]
            if title_key in seen_titles:
                continue
            
            seen_urls.add(article.url)
            seen_titles.add(title_key)
            unique_articles.append(article)
        
        return unique_articles

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
    print("ğŸš€ Daily AI News - æ‹¡å¼µç‰ˆï¼ˆå¤šè¨€èªãƒ»å¤šã‚½ãƒ¼ã‚¹å¯¾å¿œï¼‰")
    print("=" * 70)
    
    settings = Settings()
    
    # æ‹¡å¼µã‚½ãƒ¼ã‚¹åé›†
    collector = EnhancedAINewsCollector()
    articles = collector.collect_all_sources()
    
    if not articles:
        print("âŒ è¨˜äº‹ãŒåé›†ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return False
    
    # æ‹¡å¼µè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
    for article in articles:
        content_lower = (article.title + " " + article.content).lower()
        
        # è¨€èªåˆ¥è©•ä¾¡èª¿æ•´
        if 'ja' in article.tags:
            base_eng = 0.6
            base_bus = 0.5
        else:  # è‹±èªè¨˜äº‹
            base_eng = 0.7
            base_bus = 0.6
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒœãƒ¼ãƒŠã‚¹
        if 'ai_company' in article.tags:
            base_eng += 0.15
            base_bus += 0.2
        elif 'ai_research' in article.tags:
            base_eng += 0.2
            base_bus += 0.1
        elif 'tutorial' in article.tags:
            base_eng += 0.1
            base_bus += 0.05
        
        # æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒœãƒ¼ãƒŠã‚¹
        tech_keywords = ['github', 'code', 'api', 'model', 'algorithm', 'paper', 'arxiv']
        tech_bonus = min(0.2, sum(0.03 for kw in tech_keywords if kw in content_lower))
        
        # ãƒ“ã‚¸ãƒã‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒœãƒ¼ãƒŠã‚¹
        biz_keywords = ['startup', 'funding', 'enterprise', 'business', 'market', 'revenue']
        biz_bonus = min(0.2, sum(0.03 for kw in biz_keywords if kw in content_lower))
        
        article.evaluation = {
            "engineer": {"total_score": min(1.0, base_eng + tech_bonus)},
            "business": {"total_score": min(1.0, base_bus + biz_bonus)}
        }
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        is_implementable = any(kw in content_lower for kw in ['github', 'api', 'tutorial', 'howto'])
        
        article.technical = TechnicalMetadata(
            implementation_ready=is_implementable,
            code_available='github' in content_lower,
            reproducibility_score=0.8 if 'research' in article.tags else 0.6
        )
        
        complexity = ImplementationCost.LOW if 'tutorial' in article.tags else ImplementationCost.MEDIUM
        if 'research' in article.tags:
            complexity = ImplementationCost.HIGH
        
        article.business = BusinessMetadata(
            market_size="ã‚°ãƒ­ãƒ¼ãƒãƒ«" if 'en' in article.tags else "æ—¥æœ¬ä¸­å¿ƒ",
            growth_rate=article.evaluation["business"]["total_score"] * 100,
            implementation_cost=complexity
        )
    
    # ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
    articles.sort(key=lambda x: (x.evaluation["engineer"]["total_score"] + x.evaluation["business"]["total_score"]) / 2, reverse=True)
    
    # ãƒˆãƒƒãƒ—è¨˜äº‹è¡¨ç¤º
    print(f"\nğŸ† ãƒˆãƒƒãƒ—10è¨˜äº‹:")
    for i, article in enumerate(articles[:10]):
        eng_score = article.evaluation["engineer"]["total_score"]
        bus_score = article.evaluation["business"]["total_score"]
        lang_icon = "ğŸ‡¯ğŸ‡µ" if 'ja' in article.tags else "ğŸŒ"
        tier_icon = "â­" if article.source_tier == 1 else "âš¡"
        
        print(f"  {i+1:2}. {lang_icon}{tier_icon} {article.title[:55]}")
        print(f"      ã‚½ãƒ¼ã‚¹: {article.source}")
        print(f"      ã‚¹ã‚³ã‚¢: ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ {eng_score:.2f} | ãƒ“ã‚¸ãƒã‚¹ {bus_score:.2f}")
        print()
    
    # ã‚µã‚¤ãƒˆç”Ÿæˆ
    try:
        generator = StaticSiteGenerator(settings)
        
        docs_dir = Path("docs")
        docs_dir.mkdir(exist_ok=True)
        
        # ã‚¢ã‚»ãƒƒãƒˆç”Ÿæˆ
        css_content = generator.assets.generate_css()
        (docs_dir / "styles.css").write_text(css_content, encoding='utf-8')
        
        js_content = generator.assets.generate_javascript()
        (docs_dir / "script.js").write_text(js_content, encoding='utf-8')
        
        # HTMLç”Ÿæˆ
        html_generator = generator.html_generator
        processed_articles = html_generator._process_articles(articles, "engineer")
        summary_stats = html_generator._generate_summary_stats(articles)
        
        articles_html = html_generator._render_articles_grid(processed_articles, "engineer")
        filters_html = html_generator._create_interactive_filters(html_generator._extract_filter_options(articles))
        stats_html = html_generator._render_summary_stats(summary_stats)
        
        dashboard_template = html_generator.template_engine.load_template("dashboard.html")
        dashboard_content = html_generator.template_engine.render(dashboard_template, {
            "articles": articles_html,
            "filters": filters_html,
            "summary_stats": stats_html
        })
        
        page_content = html_generator._generate_complete_page(
            dashboard_content,
            title="Daily AI News - æ‹¡å¼µç‰ˆï¼ˆå¤šè¨€èªãƒ»å¤šã‚½ãƒ¼ã‚¹ï¼‰",
            description="æ—¥æœ¬èªãƒ»è‹±èªã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’40+ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†",
            persona="engineer"
        )
        
        index_file = docs_dir / "index.html"
        index_file.write_text(page_content, encoding='utf-8')
        
        print(f"\nâœ… æ‹¡å¼µAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆç”Ÿæˆå®Œäº†!")
        print(f"ğŸ“‚ å ´æ‰€: {index_file.absolute()}")
        print(f"ğŸŒ å¤šè¨€èªãƒ»å¤šã‚½ãƒ¼ã‚¹å¯¾å¿œã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å®Œæˆï¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚µã‚¤ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    
    if success:
        print("\nğŸ‰ æ‹¡å¼µAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆãŒå®Œæˆã—ã¾ã—ãŸï¼")
        print("ğŸ’¡ ç‰¹å¾´:")
        print("  ğŸŒ 40+ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®å¤šè¨€èªåé›†")
        print("  ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªAIãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆITmediaã€Gizmodoç­‰ï¼‰")
        print("  ğŸŒ è‹±èªAIãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆOpenAIã€MITç­‰ï¼‰")
        print("  ğŸ” å¼·åŒ–ã•ã‚ŒãŸAIé–¢é€£ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°")
        print("  ğŸ“Š å¤šå±¤è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ")
        print("  ğŸ¯ ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒ»ãƒ“ã‚¸ãƒã‚¹ä¸¡è¦–ç‚¹å¯¾å¿œ")
    else:
        print("\nâš ï¸ å®Ÿè¡Œã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")
    
    input("\nEnterã‚’æŠ¼ã—ã¦çµ‚äº†...")