#!/usr/bin/env python3
"""Xï¼ˆæ—§Twitterï¼‰ã®AIé–¢é€£ãƒã‚¹ãƒˆã‚’åé›†"""

import sys
import asyncio
import requests
import feedparser
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any
import time
import re
import json
from urllib.parse import quote_plus

# Add src to path
project_root = Path(__file__).parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

from src.config.settings import Settings
from src.models.article import Article, TechnicalMetadata, BusinessMetadata, ImplementationCost
from src.generators.static_site_generator import StaticSiteGenerator

class XPostCollector:
    """Xï¼ˆæ—§Twitterï¼‰ã®AIé–¢é€£ãƒã‚¹ãƒˆåé›†"""
    
    def __init__(self):
        # RSSå½¢å¼ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªXã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
        self.x_sources = {
            # AIä¼æ¥­ãƒ»ç ”ç©¶æ©Ÿé–¢ã®å…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
            "openai": {
                "url": "https://nitter.net/OpenAI/rss",
                "backup_url": "https://twitrss.me/twitter_user_to_rss/?user=OpenAI",
                "tier": 1,
                "source_name": "OpenAI (X)",
                "category": "ai_company",
                "account": "@OpenAI"
            },
            "anthropic": {
                "url": "https://nitter.net/AnthropicAI/rss", 
                "backup_url": "https://twitrss.me/twitter_user_to_rss/?user=AnthropicAI",
                "tier": 1,
                "source_name": "Anthropic (X)",
                "category": "ai_company",
                "account": "@AnthropicAI"
            },
            "google_ai": {
                "url": "https://nitter.net/GoogleAI/rss",
                "backup_url": "https://twitrss.me/twitter_user_to_rss/?user=GoogleAI",
                "tier": 1,
                "source_name": "Google AI (X)",
                "category": "ai_company", 
                "account": "@GoogleAI"
            },
            "deepmind": {
                "url": "https://nitter.net/DeepMind/rss",
                "backup_url": "https://twitrss.me/twitter_user_to_rss/?user=DeepMind",
                "tier": 1,
                "source_name": "DeepMind (X)",
                "category": "ai_company",
                "account": "@DeepMind"
            },
            # AIç ”ç©¶è€…ãƒ»ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼
            "ylecun": {
                "url": "https://nitter.net/ylecun/rss",
                "backup_url": "https://twitrss.me/twitter_user_to_rss/?user=ylecun",
                "tier": 1,
                "source_name": "Yann LeCun (X)",
                "category": "ai_researcher",
                "account": "@ylecun"
            },
            "karpathy": {
                "url": "https://nitter.net/karpathy/rss",
                "backup_url": "https://twitrss.me/twitter_user_to_rss/?user=karpathy",
                "tier": 1,
                "source_name": "Andrej Karpathy (X)",
                "category": "ai_researcher",
                "account": "@karpathy"
            },
            # æ—¥æœ¬ã®AIé–¢é€£ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
            "preferred_networks": {
                "url": "https://nitter.net/PreferredNet/rss",
                "backup_url": "https://twitrss.me/twitter_user_to_rss/?user=PreferredNet",
                "tier": 1,
                "source_name": "Preferred Networks (X)",
                "category": "ai_company_jp",
                "account": "@PreferredNet"
            },
            "rinna_inc": {
                "url": "https://nitter.net/rinna_inc/rss", 
                "backup_url": "https://twitrss.me/twitter_user_to_rss/?user=rinna_inc",
                "tier": 1,
                "source_name": "rinna (X)",
                "category": "ai_company_jp",
                "account": "@rinna_inc"
            },
            # AIç³»ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æ¤œç´¢ï¼ˆä»£æ›¿æ‰‹æ³•ï¼‰
            "ai_hashtag": {
                "url": "https://twitrss.me/twitter_search_to_rss/?term=%23AI%20OR%20%23MachineLearning%20OR%20%23DeepLearning",
                "tier": 2,
                "source_name": "AI Hashtag Posts (X)",
                "category": "hashtag_search",
                "search_terms": "#AI #MachineLearning #DeepLearning"
            }
        }
        
        # ä»£æ›¿RSSã‚µãƒ¼ãƒ“ã‚¹
        self.rss_services = [
            "https://nitter.net/{}/rss",
            "https://twitrss.me/twitter_user_to_rss/?user={}",
            "https://twitterrss.me/user/{}/feed",
            "https://rsshub.app/twitter/user/{}"
        ]
    
    def get_working_x_url(self, account_config: Dict) -> str:
        """å‹•ä½œã™ã‚‹Xã®RSS URLã‚’å–å¾—"""
        urls_to_try = []
        
        # è¨­å®šã•ã‚ŒãŸURLã‚’è¿½åŠ 
        if "url" in account_config:
            urls_to_try.append(account_config["url"])
        if "backup_url" in account_config:
            urls_to_try.append(account_config["backup_url"])
        
        # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåãŒã‚ã‚‹å ´åˆã¯å„ã‚µãƒ¼ãƒ“ã‚¹ã‚’è©¦ã™
        if "account" in account_config:
            username = account_config["account"].replace("@", "")
            for service_template in self.rss_services:
                if "{}" in service_template:
                    urls_to_try.append(service_template.format(username))
        
        # å„URLã‚’è©¦ã™
        for url in urls_to_try:
            try:
                response = requests.head(url, timeout=10)
                if response.status_code == 200:
                    print(f"  âœ… å‹•ä½œURLç™ºè¦‹: {url[:50]}...")
                    return url
            except:
                continue
        
        # ã©ã‚Œã‚‚ãƒ€ãƒ¡ãªã‚‰å…ƒã®URLã‚’è¿”ã™
        return account_config.get("url", "")
    
    def collect_from_x_feed(self, source_config: Dict, max_posts: int = 5) -> List[Article]:
        """Xã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æŠ•ç¨¿ã‚’åé›†"""
        articles = []
        
        try:
            print(f"ğŸ¦ Xåé›†ä¸­: {source_config['source_name']}...")
            
            # å‹•ä½œã™ã‚‹URLã‚’å–å¾—
            working_url = self.get_working_x_url(source_config)
            if not working_url:
                print(f"  âš ï¸ åˆ©ç”¨å¯èƒ½ãªURLç„¡ã—: {source_config['source_name']}")
                return articles
            
            headers = {
                'User-Agent': 'DailyAINews/1.0 (Educational Project)',
                'Accept': 'application/rss+xml, application/xml, text/xml',
                'Accept-Language': 'en,ja;q=0.9'
            }
            
            response = requests.get(working_url, headers=headers, timeout=20)
            
            if response.status_code != 200:
                print(f"  âš ï¸ HTTP {response.status_code}: {source_config['source_name']}")
                return articles
            
            # RSSè§£æ
            feed = feedparser.parse(response.content)
            
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                print(f"  âš ï¸ ã‚¨ãƒ³ãƒˆãƒªãƒ¼ç„¡ã—: {source_config['source_name']}")
                return articles
            
            print(f"  ğŸ“„ {len(feed.entries)}å€‹ã®æŠ•ç¨¿ã‚’ç™ºè¦‹")
            
            for entry in feed.entries[:max_posts]:
                try:
                    # æŠ•ç¨¿æ—¥æ™‚
                    post_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        post_date = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        post_date = datetime(*entry.updated_parsed[:6])
                    else:
                        post_date = datetime.now()
                    
                    # 24æ™‚é–“ä»¥å†…ã®æŠ•ç¨¿ã®ã¿
                    if post_date < datetime.now() - timedelta(days=1):
                        continue
                    
                    # æŠ•ç¨¿å†…å®¹å–å¾—
                    content = ""
                    title = entry.title if hasattr(entry, 'title') else ""
                    
                    if hasattr(entry, 'summary'):
                        content = entry.summary
                    elif hasattr(entry, 'description'):
                        content = entry.description
                    
                    # HTMLã‚¿ã‚°å‰Šé™¤
                    content = re.sub(r'<[^>]+>', '', content)
                    content = content.strip()
                    
                    # çŸ­ã™ãã‚‹æŠ•ç¨¿ã¯ã‚¹ã‚­ãƒƒãƒ—
                    if len(content) < 20:
                        continue
                    
                    # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                    ai_keywords = [
                        'AI', 'artificial intelligence', 'machine learning', 'ML', 
                        'deep learning', 'neural network', 'ChatGPT', 'GPT',
                        'LLM', 'transformer', 'NLP', 'computer vision',
                        'äººå·¥çŸ¥èƒ½', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'AIæŠ€è¡“'
                    ]
                    
                    full_text = (title + " " + content).lower()
                    if not any(keyword.lower() in full_text for keyword in ai_keywords):
                        continue
                    
                    # RTï¼ˆãƒªãƒ„ã‚¤ãƒ¼ãƒˆï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
                    if content.startswith('RT @') or 'RT @' in content[:10]:
                        continue
                    
                    # è¨˜äº‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
                    article = Article(
                        id=f"x_{source_config['source_name'].lower().replace(' ', '_').replace('(', '').replace(')', '')}_{hash(entry.link) % 10000}",
                        title=title[:100] + "..." if len(title) > 100 else title,
                        url=entry.link,
                        source=source_config['source_name'],
                        source_tier=source_config['tier'],
                        published_date=post_date,
                        content=content[:300] + "..." if len(content) > 300 else content,
                        tags=['x_post', 'ai', source_config.get('category', 'general')]
                    )
                    
                    articles.append(article)
                    print(f"    âœ… åé›†: {title[:40]}...")
                    
                except Exception as e:
                    print(f"    âš ï¸ æŠ•ç¨¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}")
                    continue
            
            print(f"  âœ… {source_config['source_name']}: {len(articles)}æŠ•ç¨¿åé›†å®Œäº†")
            
        except Exception as e:
            print(f"  âŒ {source_config['source_name']}åé›†ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}")
        
        return articles
    
    def collect_all_x_posts(self) -> List[Article]:
        """å…¨Xï¼ˆæ—§Twitterï¼‰ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†"""
        all_posts = []
        
        print("ğŸ¦ Xï¼ˆæ—§Twitterï¼‰AIæŠ•ç¨¿åé›†é–‹å§‹")
        print("-" * 50)
        
        for source_key, config in self.x_sources.items():
            posts = self.collect_from_x_feed(config)
            all_posts.extend(posts)
            time.sleep(3)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿
        
        # é‡è¤‡å‰Šé™¤
        unique_posts = []
        seen_content = set()
        seen_urls = set()
        
        for post in all_posts:
            # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯
            if post.url in seen_urls:
                continue
            
            # å†…å®¹é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€åˆã®50æ–‡å­—ï¼‰
            content_key = post.content[:50].lower().replace(' ', '')
            if content_key in seen_content:
                continue
            
            seen_urls.add(post.url)
            seen_content.add(content_key)
            unique_posts.append(post)
        
        print(f"\nğŸ“Š Xåé›†çµæœ:")
        print(f"  â€¢ ç·æŠ•ç¨¿æ•°: {len(all_posts)}")
        print(f"  â€¢ é‡è¤‡é™¤å»å¾Œ: {len(unique_posts)}")
        
        return unique_posts

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œï¼ˆæ—¥æœ¬èªã‚½ãƒ¼ã‚¹ + XæŠ•ç¨¿çµ±åˆï¼‰"""
    print("ğŸŒ Daily AI News - æ—¥æœ¬èª + XæŠ•ç¨¿çµ±åˆç‰ˆ")
    print("=" * 60)
    
    settings = Settings()
    
    # 1. æ—¥æœ¬èªã‚½ãƒ¼ã‚¹åé›†
    print("1ï¸âƒ£ æ—¥æœ¬èªã‚½ãƒ¼ã‚¹åé›†ä¸­...")
    from collect_japanese_sources import JapaneseSourceCollector
    japanese_collector = JapaneseSourceCollector()
    japanese_articles = japanese_collector.collect_all_japanese()
    
    # 2. XæŠ•ç¨¿åé›†
    print("\n2ï¸âƒ£ Xï¼ˆæ—§Twitterï¼‰æŠ•ç¨¿åé›†ä¸­...")
    x_collector = XPostCollector()
    x_posts = x_collector.collect_all_x_posts()
    
    # 3. çµ±åˆ
    all_articles = japanese_articles + x_posts
    
    if not all_articles:
        print("âŒ è¨˜äº‹ãƒ»æŠ•ç¨¿ãŒåé›†ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return False
    
    print(f"\nğŸ“ˆ çµ±åˆçµæœ:")
    print(f"  â€¢ æ—¥æœ¬èªè¨˜äº‹: {len(japanese_articles)}ä»¶")
    print(f"  â€¢ XæŠ•ç¨¿: {len(x_posts)}ä»¶") 
    print(f"  â€¢ ç·åˆè¨ˆ: {len(all_articles)}ä»¶")
    
    # 4. è©•ä¾¡
    for article in all_articles:
        content_lower = (article.title + " " + article.content).lower()
        
        # XæŠ•ç¨¿ç”¨ã®è©•ä¾¡èª¿æ•´
        if 'x_post' in article.tags:
            base_score = 0.6  # XæŠ•ç¨¿ã¯ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢é«˜ã‚
            tech_bonus = 0.2 if any(k in content_lower for k in ['github', 'code', 'paper', 'arxiv']) else 0.1
            business_bonus = 0.1 if any(k in content_lower for k in ['funding', 'startup', 'enterprise']) else 0.05
        else:
            # é€šå¸¸è¨˜äº‹ã®è©•ä¾¡
            base_score = 0.5
            tech_bonus = 0.3 if 'ai' in content_lower else 0.1
            business_bonus = 0.2 if 'business' in content_lower else 0.1
        
        article.evaluation = {
            "engineer": {"total_score": min(1.0, base_score + tech_bonus)},
            "business": {"total_score": min(1.0, base_score + business_bonus)}
        }
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®š
        article.technical = TechnicalMetadata(
            implementation_ready='x_post' in article.tags,
            code_available='github' in content_lower,
            reproducibility_score=0.8 if 'x_post' in article.tags else 0.6
        )
        
        article.business = BusinessMetadata(
            market_size="ã‚°ãƒ­ãƒ¼ãƒãƒ«" if 'x_post' in article.tags else "æ—¥æœ¬",
            growth_rate=article.evaluation["business"]["total_score"] * 100,
            implementation_cost=ImplementationCost.LOW if 'x_post' in article.tags else ImplementationCost.MEDIUM
        )
    
    # 5. ã‚½ãƒ¼ãƒˆï¼ˆã‚¹ã‚³ã‚¢é †ï¼‰
    all_articles.sort(key=lambda x: (x.evaluation["engineer"]["total_score"] + x.evaluation["business"]["total_score"]) / 2, reverse=True)
    
    # 6. ãƒˆãƒƒãƒ—è¨˜äº‹è¡¨ç¤º
    print(f"\nğŸ† ãƒˆãƒƒãƒ—è¨˜äº‹ãƒ»æŠ•ç¨¿:")
    for i, article in enumerate(all_articles[:5]):
        eng_score = article.evaluation["engineer"]["total_score"]
        bus_score = article.evaluation["business"]["total_score"]
        post_type = "ğŸ“±" if 'x_post' in article.tags else "ğŸ“°"
        print(f"  {i+1}. {post_type} {article.title[:60]}")
        print(f"     ã‚½ãƒ¼ã‚¹: {article.source}")
        print(f"     ã‚¹ã‚³ã‚¢: ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ {eng_score:.2f} | ãƒ“ã‚¸ãƒã‚¹ {bus_score:.2f}")
        print()
    
    # 7. ã‚µã‚¤ãƒˆç”Ÿæˆ
    try:
        generator = StaticSiteGenerator(settings)
        
        docs_dir = Path("docs")
        docs_dir.mkdir(exist_ok=True)
        
        # ã‚¢ã‚»ãƒƒãƒˆç”Ÿæˆ
        css_content = generator.assets.generate_css()
        (docs_dir / "styles.css").write_text(css_content, encoding='utf-8')
        
        js_content = generator.assets.generate_javascript()
        (docs_dir / "script.js").write_text(js_content, encoding='utf-8')
        
        # HTMLç”Ÿæˆ
        html_generator = generator.html_generator
        processed_articles = html_generator._process_articles(all_articles, "engineer")
        summary_stats = html_generator._generate_summary_stats(all_articles)
        
        articles_html = html_generator._render_articles_grid(processed_articles, "engineer")
        filters_html = html_generator._create_interactive_filters(html_generator._extract_filter_options(all_articles))
        stats_html = html_generator._render_summary_stats(summary_stats)
        
        dashboard_template = html_generator.template_engine.load_template("dashboard.html")
        dashboard_content = html_generator.template_engine.render(dashboard_template, {
            "articles": articles_html,
            "filters": filters_html,
            "summary_stats": stats_html
        })
        
        page_content = html_generator._generate_complete_page(
            dashboard_content,
            title="Daily AI News - æ—¥æœ¬èª + XæŠ•ç¨¿çµ±åˆç‰ˆ",
            description="æ—¥æœ¬èªAIãƒ‹ãƒ¥ãƒ¼ã‚¹ + Xï¼ˆæ—§Twitterï¼‰æŠ•ç¨¿ã®çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
            persona="engineer"
        )
        
        index_file = docs_dir / "index.html"
        index_file.write_text(page_content, encoding='utf-8')
        
        print(f"\nâœ… çµ±åˆAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆç”Ÿæˆå®Œäº†!")
        print(f"ğŸ“‚ å ´æ‰€: {index_file.absolute()}")
        print(f"ğŸŒ æ—¥æœ¬èªè¨˜äº‹ + XæŠ•ç¨¿ãŒçµ±åˆã•ã‚Œã¾ã—ãŸï¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚µã‚¤ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    
    if success:
        print("\nğŸ‰ æ—¥æœ¬èªAIãƒ‹ãƒ¥ãƒ¼ã‚¹ + XæŠ•ç¨¿ã®çµ±åˆã‚µã‚¤ãƒˆãŒå®Œæˆã—ã¾ã—ãŸï¼")
        print("ğŸ’¡ XæŠ•ç¨¿ã¯ğŸ“±ã‚¢ã‚¤ã‚³ãƒ³ã§ã€è¨˜äº‹ã¯ğŸ“°ã‚¢ã‚¤ã‚³ãƒ³ã§è¡¨ç¤ºã•ã‚Œã¾ã™")
    else:
        print("\nâš ï¸ å®Ÿè¡Œã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")
    
    input("\nEnterã‚’æŠ¼ã—ã¦çµ‚äº†...")