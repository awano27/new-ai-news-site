#!/usr/bin/env python3
"""
2025å¹´å¯¾å¿œ ç°¡å˜ç‰ˆAIæƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ 
æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿ã§å‹•ä½œ
"""

import sys
import asyncio
import requests
import feedparser
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any
import time
import re
import hashlib

# Add src to path
project_root = Path(__file__).parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

from src.config.settings import Settings
from src.models.article import Article, TechnicalMetadata, BusinessMetadata, ImplementationCost
from src.generators.static_site_generator import StaticSiteGenerator
from x_source_collector_local import XSourceCollectorLocal
from x_source_collector import XSourceCollector

class Simple2025AICollector:
    """2025å¹´å¯¾å¿œç°¡å˜ç‰ˆAIæƒ…å ±åé›†"""
    
    def __init__(self):
        self.sources = {
            # é«˜ä¿¡é ¼æ€§ã‚½ãƒ¼ã‚¹ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
            "reddit_ml": {
                "url": "https://www.reddit.com/r/MachineLearning/.rss",
                "tier": 1,
                "source_name": "Reddit MachineLearning",
                "category": "community",
                "lang": "en"
            },
            "reddit_localllama": {
                "url": "https://www.reddit.com/r/LocalLLaMA/.rss",
                "tier": 1,
                "source_name": "Reddit LocalLLaMA",
                "category": "community",
                "lang": "en"
            },
            "mit_tech_review": {
                "url": "https://www.technologyreview.com/topic/artificial-intelligence/feed/",
                "tier": 1,
                "source_name": "MIT Technology Review AI",
                "category": "tech_media",
                "lang": "en"
            },
            "venturebeat_ai": {
                "url": "https://venturebeat.com/category/ai/feed/",
                "tier": 1,
                "source_name": "VentureBeat AI",
                "category": "tech_media",
                "lang": "en"
            },
            "techcrunch_ai": {
                "url": "https://techcrunch.com/category/artificial-intelligence/feed/",
                "tier": 1,
                "source_name": "TechCrunch AI",
                "category": "tech_media",
                "lang": "en"
            },
            "the_verge_ai": {
                "url": "https://www.theverge.com/rss/ai-artificial-intelligence",
                "tier": 1,
                "source_name": "The Verge AI",
                "category": "tech_media",
                "lang": "en"
            },
            
            # æ—¥æœ¬èªã‚½ãƒ¼ã‚¹
            "itmedia_ai": {
                "url": "https://rss.itmedia.co.jp/rss/2.0/ait.xml",
                "tier": 1,
                "source_name": "ITmedia AI",
                "category": "tech_media_jp",
                "lang": "ja"
            },
            "itmedia_news_tech": {
                "url": "https://rss.itmedia.co.jp/rss/2.0/news_technology.xml",
                "tier": 1,
                "source_name": "ITmedia News Tech",
                "category": "tech_media_jp",
                "lang": "ja"
            },
            "internet_watch": {
                "url": "https://internet.watch.impress.co.jp/data/rss/1.0/iw/feed.rdf",
                "tier": 1,
                "source_name": "INTERNET Watch",
                "category": "tech_media_jp",
                "lang": "ja"
            },
            "gizmodo_jp": {
                "url": "https://www.gizmodo.jp/index.xml",
                "tier": 1,
                "source_name": "Gizmodo Japan",
                "category": "tech_media_jp",
                "lang": "ja"
            },
            "cnet_japan": {
                "url": "http://feeds.japan.cnet.com/rss/cnet/all.rdf",
                "tier": 1,
                "source_name": "CNET Japan",
                "category": "tech_media_jp",
                "lang": "ja"
            },
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼
            "import_ai": {
                "url": "https://importai.substack.com/feed",
                "tier": 1,
                "source_name": "Import AI",
                "category": "newsletter",
                "lang": "en"
            }
        }
    
    def enhance_ai_filtering_2025(self, content: str, title: str) -> bool:
        """2025å¹´å¯¾å¿œAIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        text = (title + " " + content).lower()
        
        # 2025å¹´æœ€æ–°AIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        ai_keywords_2025 = [
            # åŸºæœ¬AIç”¨èª
            'artificial intelligence', 'ai', 'machine learning', 'ml', 'deep learning',
            'neural network', 'neural net',
            
            # å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«
            'llm', 'large language model', 'chatgpt', 'gpt-4', 'gpt-5',
            'claude', 'gemini', 'bard', 'llama', 'mistral',
            
            # ç”ŸæˆAI
            'generative ai', 'gen ai', 'text generation', 'image generation',
            'stable diffusion', 'midjourney', 'dall-e', 'sora',
            
            # 2025å¹´æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰
            'rag', 'retrieval augmented generation', 'fine-tuning',
            'prompt engineering', 'multimodal', 'ai agent',
            'transformer', 'attention', 'foundation model',
            
            # æ—¥æœ¬èªAIç”¨èª
            'äººå·¥çŸ¥èƒ½', 'AI', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°',
            'ãƒãƒ£ãƒƒãƒˆGPT', 'LLM', 'ç”ŸæˆAI', 'AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ',
            'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ', 'ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°', 'RAG'
        ]
        
        return any(keyword.lower() in text for keyword in ai_keywords_2025)
    
    def collect_from_feed(self, feed_config: Dict, max_articles: int = 4) -> List[Article]:
        """ç°¡å˜ç‰ˆãƒ•ã‚£ãƒ¼ãƒ‰åé›†"""
        articles = []
        
        try:
            print(f"ğŸ“¡ åé›†ä¸­: {feed_config['source_name']}...")
            
            headers = {
                'User-Agent': 'DailyAINews/2.0 (Educational Project)',
                'Accept': 'application/rss+xml, text/xml'
            }
            
            response = requests.get(feed_config['url'], headers=headers, timeout=15)
            
            if response.status_code != 200:
                print(f"  âš ï¸ HTTP {response.status_code}")
                return articles
            
            # ç°¡å˜ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†
            try:
                if 'japan' in feed_config['url'].lower() or '.jp' in feed_config['url']:
                    try:
                        content = response.content.decode('utf-8')
                    except:
                        content = response.content.decode('shift-jis', errors='ignore')
                else:
                    content = response.text
            except:
                content = response.content.decode('utf-8', errors='ignore')
            
            feed = feedparser.parse(content)
            
            if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                print(f"  âš ï¸ ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãªã—")
                return articles
            
            print(f"  ğŸ“„ {len(feed.entries)}å€‹ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’ç™ºè¦‹")
            
            for entry in feed.entries[:max_articles * 2]:
                try:
                    # æ—¥ä»˜å‡¦ç†
                    pub_date = datetime.now()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        pub_date = datetime(*entry.updated_parsed[:6])
                    
                    # 3æ—¥ä»¥å†…ã®è¨˜äº‹ã®ã¿
                    if pub_date < datetime.now() - timedelta(days=3):
                        continue
                    
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
                    title = getattr(entry, 'title', '')
                    content = getattr(entry, 'summary', '') or getattr(entry, 'description', '')
                    
                    # HTMLã‚¿ã‚°å‰Šé™¤
                    content = re.sub(r'<[^>]+>', '', content)
                    content = re.sub(r'\s+', ' ', content).strip()
                    
                    # æœ€ä½æ–‡å­—æ•°
                    if len(content) < 30:
                        continue
                    
                    # AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if not self.enhance_ai_filtering_2025(content, title):
                        continue
                    
                    # è¨˜äº‹ä½œæˆ
                    article = Article(
                        id=f"{feed_config['category']}_{hashlib.md5((title + feed_config['source_name']).encode()).hexdigest()[:8]}",
                        title=title[:150],
                        url=getattr(entry, 'link', ''),
                        source=feed_config['source_name'],
                        source_tier=feed_config['tier'],
                        published_date=pub_date,
                        content=content[:400] + "..." if len(content) > 400 else content,
                        tags=[feed_config['category'], feed_config['lang'], 'ai_2025']
                    )
                    
                    articles.append(article)
                    print(f"    âœ… åé›†: {title[:50]}...")
                    
                    if len(articles) >= max_articles:
                        break
                    
                except Exception as e:
                    print(f"    âš ï¸ ã‚¨ãƒ³ãƒˆãƒªãƒ¼å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}")
                    continue
            
            print(f"  âœ… {feed_config['source_name']}: {len(articles)}è¨˜äº‹åé›†å®Œäº†")
            
        except Exception as e:
            print(f"  âŒ {feed_config['source_name']}åé›†ã‚¨ãƒ©ãƒ¼: {str(e)[:60]}")
        
        return articles
    
    def collect_all(self) -> List[Article]:
        """å…¨ã‚½ãƒ¼ã‚¹åé›†"""
        all_articles = []
        
        print("ğŸš€ 2025å¹´å¯¾å¿œ ç°¡å˜ç‰ˆAIæƒ…å ±åé›†é–‹å§‹")
        print("=" * 50)
        
        for source_key, config in self.sources.items():
            articles = self.collect_from_feed(config)
            all_articles.extend(articles)
            time.sleep(2)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
        
        # é‡è¤‡å‰Šé™¤
        unique_articles = []
        seen_urls = set()
        seen_titles = set()
        
        for article in all_articles:
            if article.url in seen_urls:
                continue
            
            title_key = re.sub(r'[^\w\s]', '', article.title.lower())[:50]
            if title_key in seen_titles:
                continue
            
            seen_urls.add(article.url)
            seen_titles.add(title_key)
            unique_articles.append(article)
        
        print(f"\nğŸ“Š ç°¡å˜ç‰ˆåé›†çµæœ:")
        print(f"  â€¢ ç·è¨˜äº‹æ•°: {len(all_articles)}")
        print(f"  â€¢ é‡è¤‡é™¤å»å¾Œ: {len(unique_articles)}")
        print(f"  â€¢ æ—¥æœ¬èªè¨˜äº‹: {len([a for a in unique_articles if 'ja' in a.tags])}")
        print(f"  â€¢ è‹±èªè¨˜äº‹: {len([a for a in unique_articles if 'en' in a.tags])}")
        
        return unique_articles

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œï¼ˆç°¡å˜ç‰ˆï¼‰"""
    print("âš¡ Daily AI News - 2025å¹´å¯¾å¿œç°¡å˜ç‰ˆ")
    print("æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿ã§å‹•ä½œ")
    print("=" * 50)
    
    settings = Settings()
    
    # ç°¡å˜ç‰ˆåé›†
    collector = Simple2025AICollector()
    articles = collector.collect_all()
    
    # Xè¨˜äº‹åé›†ã‚‚è¿½åŠ ï¼ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ã¿ï¼‰
    print("\nğŸ¦ Xè¨˜äº‹åé›†ã‚’é–‹å§‹...")
    
    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰ç›´æ¥å–å¾—ã®ã¿
    x_collector = XSourceCollector()
    x_articles = x_collector.parse_x_articles()
    
    if x_articles:
        print(f"âœ… æœ¬ç‰©ã®Xè¨˜äº‹ {len(x_articles)}ä»¶ã‚’è¿½åŠ ")
        articles.extend(x_articles)
    else:
        print("âš ï¸ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰Xè¨˜äº‹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
    
    if not articles:
        print("âŒ è¨˜äº‹ãŒåé›†ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return False
    
    # ç°¡å˜è©•ä¾¡
    for article in articles:
        content_lower = (article.title + " " + article.content).lower()
        
        # åŸºæœ¬ã‚¹ã‚³ã‚¢
        base_eng = 0.7 if 'en' in article.tags else 0.6
        base_bus = 0.6 if 'en' in article.tags else 0.5
        
        # 2025å¹´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒœãƒ¼ãƒŠã‚¹
        keywords_2025 = ['rag', 'multimodal', 'agent', 'fine-tuning', 'gpt-4', 'claude', 'gemini']
        bonus = min(0.2, sum(0.05 for kw in keywords_2025 if kw in content_lower))
        
        article.evaluation = {
            "engineer": {"total_score": min(1.0, base_eng + bonus)},
            "business": {"total_score": min(1.0, base_bus + bonus * 0.7)}
        }
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        article.technical = TechnicalMetadata(
            implementation_ready='tutorial' in content_lower or 'github' in content_lower,
            code_available='github' in content_lower,
            reproducibility_score=0.7
        )
        
        article.business = BusinessMetadata(
            market_size="ã‚°ãƒ­ãƒ¼ãƒãƒ«" if 'en' in article.tags else "æ—¥æœ¬",
            growth_rate=article.evaluation["business"]["total_score"] * 100,
            implementation_cost=ImplementationCost.MEDIUM
        )
    
    # ã‚½ãƒ¼ãƒˆ
    articles.sort(key=lambda x: (
        x.evaluation["engineer"]["total_score"] + 
        x.evaluation["business"]["total_score"]
    ) / 2, reverse=True)
    
    # ãƒˆãƒƒãƒ—è¨˜äº‹è¡¨ç¤º
    print(f"\nğŸ† ãƒˆãƒƒãƒ—10è¨˜äº‹:")
    for i, article in enumerate(articles[:10]):
        eng_score = article.evaluation["engineer"]["total_score"]
        bus_score = article.evaluation["business"]["total_score"]
        lang_icon = "ğŸ‡¯ğŸ‡µ" if 'ja' in article.tags else "ğŸŒ"
        
        print(f"  {i+1:2}. {lang_icon} {article.title[:50]}...")
        print(f"      ã‚½ãƒ¼ã‚¹: {article.source}")
        print(f"      ã‚¹ã‚³ã‚¢: ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ {eng_score:.2f} | ãƒ“ã‚¸ãƒã‚¹ {bus_score:.2f}")
        print()
    
    # ã‚µã‚¤ãƒˆç”Ÿæˆ
    try:
        generator = StaticSiteGenerator(settings)
        
        docs_dir = Path("docs")
        docs_dir.mkdir(exist_ok=True)
        
        # CSS/JSç”Ÿæˆ
        css_content = generator.assets.generate_css()
        (docs_dir / "styles.css").write_text(css_content, encoding='utf-8')
        
        js_content = generator.assets.generate_javascript()
        (docs_dir / "script.js").write_text(js_content, encoding='utf-8')
        
        # HTMLç”Ÿæˆ
        html_generator = generator.html_generator
        processed_articles = html_generator._process_articles(articles, "engineer")
        summary_stats = html_generator._generate_summary_stats(articles)
        
        articles_html = html_generator._render_articles_grid(processed_articles, "engineer")
        filters_html = html_generator._create_interactive_filters(html_generator._extract_filter_options(articles))
        stats_html = html_generator._render_summary_stats(summary_stats)
        
        dashboard_template = html_generator.template_engine.load_template("dashboard.html")
        dashboard_content = html_generator.template_engine.render(dashboard_template, {
            "articles": articles_html,
            "filters": filters_html,
            "summary_stats": stats_html
        })
        
        page_content = html_generator._generate_complete_page(
            dashboard_content,
            title="Daily AI News - 2025å¹´ç°¡å˜ç‰ˆ",
            description="2025å¹´æœ€æ–°AIãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿ã§åé›†",
            persona="engineer",
            articles=articles
        )
        
        index_file = docs_dir / "index.html"
        index_file.write_text(page_content, encoding='utf-8')
        
        print(f"\nâœ… 2025å¹´ç°¡å˜ç‰ˆAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆç”Ÿæˆå®Œäº†!")
        print(f"ğŸ“‚ å ´æ‰€: {index_file.absolute()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚µã‚¤ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    
    if success:
        print("\nğŸ‰ 2025å¹´ç°¡å˜ç‰ˆAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆãŒå®Œæˆã—ã¾ã—ãŸï¼")
        print("âš¡ ç‰¹å¾´:")
        print("  ğŸ“¦ æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿ã§å‹•ä½œ")
        print("  ğŸ” 2025å¹´æœ€æ–°AIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œ")
        print("  ğŸŒ è‹±èªãƒ»æ—¥æœ¬èªä¸¡å¯¾å¿œ")
        print("  ğŸ“Š Redditã€MIT Tech Reviewç­‰ã®é«˜å“è³ªã‚½ãƒ¼ã‚¹")
        print("  âš¡ RAGã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã€AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¯¾å¿œ")
    else:
        print("\nâš ï¸ å®Ÿè¡Œã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")
    
    input("\nEnterã‚’æŠ¼ã—ã¦çµ‚äº†...")