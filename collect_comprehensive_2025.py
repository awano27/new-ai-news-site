#!/usr/bin/env python3
"""
2024-2025å¹´æœ€æ–°å¯¾å¿œ åŒ…æ‹¬çš„AIæƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ 
å‚è€ƒè³‡æ–™ã€Œè¦ä»¶è¿½åŠ .txtã€ã«åŸºã¥ãå®Œå…¨å®Ÿè£…ç‰ˆ
"""

import sys
import asyncio
import requests
import feedparser
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import time
import re
import json
import hashlib
from functools import wraps
from collections import defaultdict

# Add src to path
project_root = Path(__file__).parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

from src.config.settings import Settings
from src.models.article import Article, TechnicalMetadata, BusinessMetadata, ImplementationCost
from src.generators.static_site_generator import StaticSiteGenerator

class RateLimiter:
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™å®Ÿè£…"""
    
    def __init__(self):
        self.requests = defaultdict(list)
    
    def rate_limited(self, max_requests=100, window=3600):
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                now = time.time()
                key = func.__name__
                
                # å¤ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰Šé™¤
                self.requests[key] = [
                    t for t in self.requests[key] 
                    if now - t < window
                ]
                
                if len(self.requests[key]) < max_requests:
                    self.requests[key].append(now)
                    return func(*args, **kwargs)
                else:
                    sleep_time = window / max_requests
                    time.sleep(sleep_time)
                    return func(*args, **kwargs)
            return wrapper
        return decorator

class Comprehensive2025AICollector:
    """2024-2025å¹´å®Œå…¨å¯¾å¿œAIæƒ…å ±åé›†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.limiter = RateLimiter()
        self.sources = {
            # ===== å¿…é ˆå®Ÿè£…ï¼ˆé«˜ä¿¡é ¼æ€§ï¼‰- å‚è€ƒè³‡æ–™å„ªå…ˆé †ä½1 =====
            
            # arXiv RSSï¼ˆå®Œå…¨å‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
            "arxiv_ai_combined": {
                "url": "https://rss.arxiv.org/rss/cs.AI+cs.LG+cs.CV+cs.CL",
                "tier": 1,
                "source_name": "arXiv AI Combined",
                "category": "research",
                "lang": "en",
                "update_freq": "daily",
                "reliability": 5
            },
            
            # Reddit RSSï¼ˆä¸»è¦AIã‚µãƒ–ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆï¼‰
            "reddit_ml": {
                "url": "https://www.reddit.com/r/MachineLearning/.rss",
                "tier": 1,
                "source_name": "Reddit MachineLearning",
                "category": "community",
                "lang": "en",
                "update_freq": "hourly",
                "reliability": 5
            },
            "reddit_artificial": {
                "url": "https://www.reddit.com/r/artificial/.rss",
                "tier": 2,
                "source_name": "Reddit Artificial",
                "category": "community",
                "lang": "en",
                "update_freq": "hourly",
                "reliability": 4
            },
            "reddit_localllama": {
                "url": "https://www.reddit.com/r/LocalLLaMA/.rss",
                "tier": 2,
                "source_name": "Reddit LocalLLaMA",
                "category": "community",
                "lang": "en",
                "update_freq": "hourly",
                "reliability": 4
            },
            "reddit_deeplearning": {
                "url": "https://www.reddit.com/r/deeplearning/.rss",
                "tier": 2,
                "source_name": "Reddit DeepLearning",
                "category": "community",
                "lang": "en",
                "update_freq": "daily",
                "reliability": 4
            },
            
            # æ—¥æœ¬èªã‚½ãƒ¼ã‚¹ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
            "itmedia_ai_plus": {
                "url": "https://rss.itmedia.co.jp/rss/2.0/aiplus.xml",
                "tier": 1,
                "source_name": "ITmedia AI+",
                "category": "tech_media_jp",
                "lang": "ja",
                "update_freq": "daily",
                "reliability": 4
            },
            "nikkei_xtech_it": {
                "url": "https://xtech.nikkei.com/rss/xtech-it.rdf",
                "tier": 1,
                "source_name": "æ—¥çµŒxTECH IT",
                "category": "tech_media_jp",
                "lang": "ja",
                "update_freq": "daily",
                "reliability": 4
            },
            "cnet_japan": {
                "url": "http://feeds.japan.cnet.com/rss/cnet/all.rdf",
                "tier": 1,
                "source_name": "CNET Japan",
                "category": "tech_media_jp",
                "lang": "ja",
                "update_freq": "daily",
                "reliability": 4
            },
            
            # MIT Technology Reviewï¼ˆãƒ“ã‚¸ãƒã‚¹è¦–ç‚¹ï¼‰
            "mit_tech_review": {
                "url": "https://www.technologyreview.com/topic/artificial-intelligence/feed/",
                "tier": 1,
                "source_name": "MIT Technology Review AI",
                "category": "tech_media",
                "lang": "en",
                "update_freq": "weekly",
                "reliability": 5
            },
            
            # ===== æ¨å¥¨å®Ÿè£…ï¼ˆä¸­ä¿¡é ¼æ€§ï¼‰=====
            
            # ç ”ç©¶æ©Ÿé–¢ãƒ–ãƒ­ã‚°ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ï¼‰
            "google_ai_blog": {
                "url": "https://ai.googleblog.com/feeds/posts/default",
                "tier": 1,
                "source_name": "Google AI Blog",
                "category": "ai_research",
                "lang": "en",
                "update_freq": "weekly",
                "reliability": 5
            },
            "microsoft_ai": {
                "url": "https://blogs.microsoft.com/ai/feed/",
                "tier": 1,
                "source_name": "Microsoft AI Blog",
                "category": "ai_research",
                "lang": "en",
                "update_freq": "weekly",
                "reliability": 5
            },
            "mit_ai_news": {
                "url": "https://news.mit.edu/rss/topic/artificial-intelligence",
                "tier": 1,
                "source_name": "MIT AI News",
                "category": "ai_research",
                "lang": "en",
                "update_freq": "weekly",
                "reliability": 5
            },
            "bair_blog": {
                "url": "https://bair.berkeley.edu/blog/feed.xml",
                "tier": 1,
                "source_name": "BAIR Blog",
                "category": "ai_research",
                "lang": "en",
                "update_freq": "monthly",
                "reliability": 5
            },
            
            # ãƒ“ã‚¸ãƒã‚¹ãƒ»ãƒ†ãƒƒã‚¯ãƒ¡ãƒ‡ã‚£ã‚¢
            "venturebeat_ai": {
                "url": "https://venturebeat.com/category/ai/feed/",
                "tier": 2,
                "source_name": "VentureBeat AI",
                "category": "tech_media",
                "lang": "en",
                "update_freq": "daily",
                "reliability": 4
            },
            "techcrunch_ai": {
                "url": "https://techcrunch.com/category/artificial-intelligence/feed/",
                "tier": 2,
                "source_name": "TechCrunch AI",
                "category": "tech_media",
                "lang": "en",
                "update_freq": "daily",
                "reliability": 3
            },
            "the_verge_ai": {
                "url": "https://www.theverge.com/rss/ai-artificial-intelligence",
                "tier": 2,
                "source_name": "The Verge AI",
                "category": "tech_media",
                "lang": "en",
                "update_freq": "daily",
                "reliability": 4
            },
            "wired_ai": {
                "url": "https://www.wired.com/feed/tag/ai/latest/rss",
                "tier": 2,
                "source_name": "WIRED AI",
                "category": "tech_media",
                "lang": "en",
                "update_freq": "weekly",
                "reliability": 4
            },
            "ars_technica_ai": {
                "url": "https://arstechnica.com/ai/feed/",
                "tier": 2,
                "source_name": "Ars Technica AI",
                "category": "tech_media",
                "lang": "en",
                "update_freq": "weekly",
                "reliability": 4
            },
            "zdnet_ai": {
                "url": "https://www.zdnet.com/topic/artificial-intelligence/rss.xml",
                "tier": 2,
                "source_name": "ZDNet AI",
                "category": "tech_media",
                "lang": "en",
                "update_freq": "weekly",
                "reliability": 3
            },
            
            # Substackãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼ï¼ˆRSSå¯¾å¿œï¼‰
            "import_ai": {
                "url": "https://importai.substack.com/feed",
                "tier": 1,
                "source_name": "Import AI",
                "category": "newsletter",
                "lang": "en",
                "update_freq": "weekly",
                "reliability": 5
            },
            "ahead_of_ai": {
                "url": "https://magazine.sebastianraschka.com/feed",
                "tier": 1,
                "source_name": "Ahead of AI",
                "category": "newsletter",
                "lang": "en",
                "update_freq": "weekly",
                "reliability": 5
            },
            
            # YouTube RSSï¼ˆä¸»è¦ãƒãƒ£ãƒ³ãƒãƒ«ï¼‰
            "two_minute_papers": {
                "url": "https://www.youtube.com/feeds/videos.xml?channel_id=UCbfYPyITQ-7l4upoX8nvctg",
                "tier": 2,
                "source_name": "Two Minute Papers",
                "category": "video",
                "lang": "en",
                "update_freq": "weekly",
                "reliability": 4
            },
            "lex_fridman_podcast": {
                "url": "https://lexfridman.com/feed/podcast/",
                "tier": 2,
                "source_name": "Lex Fridman Podcast",
                "category": "podcast",
                "lang": "en",
                "update_freq": "weekly",
                "reliability": 4
            },
            
            # ===== è¿½åŠ ã®æ—¥æœ¬èªã‚½ãƒ¼ã‚¹ =====
            "itmedia_ai": {
                "url": "https://rss.itmedia.co.jp/rss/2.0/ait.xml",
                "tier": 2,
                "source_name": "ITmedia AI",
                "category": "tech_media_jp",
                "lang": "ja",
                "update_freq": "daily",
                "reliability": 4
            },
            "itmedia_news_tech": {
                "url": "https://rss.itmedia.co.jp/rss/2.0/news_technology.xml",
                "tier": 2,
                "source_name": "ITmedia News Tech",
                "category": "tech_media_jp",
                "lang": "ja",
                "update_freq": "daily",
                "reliability": 4
            },
            "internet_watch": {
                "url": "https://internet.watch.impress.co.jp/data/rss/1.0/iw/feed.rdf",
                "tier": 2,
                "source_name": "INTERNET Watch",
                "category": "tech_media_jp",
                "lang": "ja",
                "update_freq": "daily",
                "reliability": 4
            },
            "pc_watch": {
                "url": "https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf",
                "tier": 2,
                "source_name": "PC Watch",
                "category": "tech_media_jp",
                "lang": "ja",
                "update_freq": "daily",
                "reliability": 4
            }
        }
    
    def parse_rss_safely(self, url: str) -> Optional[dict]:
        """å®‰å…¨ãªRSSè§£æï¼ˆæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã¿ï¼‰"""
        try:
            headers = {
                'User-Agent': 'DailyAINews/2.0 (Educational AI Research Project)',
                'Accept': 'application/rss+xml, application/xml, text/xml, application/atom+xml',
                'Accept-Language': 'ja,en;q=0.9'
            }
            
            response = requests.get(url, headers=headers, timeout=20)
            
            if response.status_code != 200:
                return None
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å–å¾—
            content_type = response.headers.get('content-type', '')
            encoding = 'utf-8'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            
            if 'charset=' in content_type:
                try:
                    encoding = content_type.split('charset=')[-1].split(';')[0].strip()
                except:
                    encoding = 'utf-8'
            
            # æ—¥æœ¬èªã‚µã‚¤ãƒˆç‰¹åˆ¥å‡¦ç†
            if 'japan' in url.lower() or '.jp' in url or 'nikkei' in url or 'itmedia' in url:
                # æ—¥æœ¬èªã‚µã‚¤ãƒˆã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è©¦è¡Œé †åº
                encodings_to_try = ['utf-8', 'shift-jis', 'euc-jp', 'iso-2022-jp']
                content = None
                
                for enc in encodings_to_try:
                    try:
                        content = response.content.decode(enc)
                        break
                    except UnicodeDecodeError:
                        continue
                
                if content is None:
                    content = response.content.decode('utf-8', errors='ignore')
            else:
                try:
                    content = response.content.decode(encoding)
                except:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚ˆãã‚ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™
                    for enc in ['utf-8', 'latin-1', 'iso-8859-1']:
                        try:
                            content = response.content.decode(enc)
                            break
                        except:
                            continue
                    else:
                        content = response.content.decode('utf-8', errors='ignore')
            
            return feedparser.parse(content)
            
        except Exception as e:
            print(f"  âŒ RSSè§£æã‚¨ãƒ©ãƒ¼: {str(e)[:80]}")
            return None
    
    def enhance_ai_filtering_2025(self, content: str, title: str, source_category: str) -> bool:
        """2025å¹´å¯¾å¿œå¼·åŒ–AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        text = (title + " " + content).lower()
        
        # 2024-2025å¹´ã®æœ€æ–°AIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        ai_keywords_2024_2025 = [
            # åŸºæœ¬AIç”¨èª
            'artificial intelligence', 'ai', 'machine learning', 'ml', 'deep learning',
            'neural network', 'neural net',
            
            # å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«é–¢é€£
            'llm', 'large language model', 'language model', 'chatgpt', 'gpt-4', 'gpt-5',
            'claude', 'gemini', 'bard', 'palm', 'llama', 'mistral', 'anthropic',
            
            # ç”ŸæˆAI
            'generative ai', 'gen ai', 'text generation', 'image generation',
            'video generation', 'stable diffusion', 'midjourney', 'dall-e',
            'sora', 'runway ml',
            
            # æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ 2024-2025
            'rag', 'retrieval augmented generation', 'fine-tuning', 'few-shot',
            'prompt engineering', 'prompt tuning', 'chain of thought',
            'multimodal', 'foundation model', 'transformer',
            'attention mechanism', 'self-attention',
            
            # AI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ»è‡ªå‹•åŒ–
            'ai agent', 'autonomous agent', 'langchain', 'autogpt',
            'multiagent system', 'ai automation',
            
            # æ¥­ç•Œãƒ»å¿œç”¨åˆ†é‡
            'computer vision', 'nlp', 'natural language processing',
            'robotics', 'autonomous vehicle', 'medical ai',
            'ai safety', 'alignment', 'agi', 'artificial general intelligence',
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ»ãƒ„ãƒ¼ãƒ«
            'pytorch', 'tensorflow', 'hugging face', 'transformers',
            'openai api', 'anthropic api', 'ollama', 'vllm',
            
            # æ—¥æœ¬èªAIç”¨èª
            'äººå·¥çŸ¥èƒ½', 'AI', 'ã‚¨ãƒ¼ã‚¢ã‚¤', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'æ·±å±¤å­¦ç¿’',
            'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯', 'è‡ªç„¶è¨€èªå‡¦ç†', 'ãƒãƒ£ãƒƒãƒˆGPT', 'ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ',
            'LLM', 'å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«', 'è¨€èªãƒ¢ãƒ‡ãƒ«', 'AIæŠ€è¡“', 'AIæ´»ç”¨', 'AIå°å…¥',
            'ç”ŸæˆAI', 'å¯¾è©±AI', 'ç”»åƒç”Ÿæˆ', 'ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ', 'ç”Ÿæˆç³»AI',
            'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ', 'ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°', 'RAG', 'ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼',
            'AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ', 'AIè‡ªå‹•åŒ–', 'AGI', 'æ±ç”¨äººå·¥çŸ¥èƒ½'
        ]
        
        # ã‚«ãƒ†ã‚´ãƒªç‰¹æœ‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼·åŒ–
        category_keywords = {
            'research': ['arxiv', 'paper', 'research', 'study', 'experiment', 'dataset', 'benchmark'],
            'community': ['reddit', 'discussion', 'community', 'open source', 'github'],
            'tech_media': ['startup', 'funding', 'product', 'release', 'announcement'],
            'newsletter': ['analysis', 'insight', 'trend', 'overview', 'weekly'],
            'video': ['tutorial', 'explanation', 'demo', 'walkthrough'],
            'podcast': ['interview', 'discussion', 'conversation']
        }
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ2025å¹´ç‰ˆï¼‰
        exclude_keywords_2025 = [
            'air conditioning', 'artificial insemination', 'adobe illustrator',
            'american idol', 'athletic identity', 'artificial intelligence movie',
            'apple intelligence', 'business intelligence', 'competitive intelligence',
            'emotional intelligence', 'multiple intelligences',
            'air india', 'amnesty international', 'adobe indesign'
        ]
        
        # é™¤å¤–ãƒã‚§ãƒƒã‚¯
        if any(exclude.lower() in text for exclude in exclude_keywords_2025):
            return False
        
        # AIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        ai_match = any(keyword.lower() in text for keyword in ai_keywords_2024_2025)
        
        # ã‚«ãƒ†ã‚´ãƒªç‰¹æœ‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        category_match = False
        if source_category in category_keywords:
            category_match = any(keyword.lower() in text for keyword in category_keywords[source_category])
        
        return ai_match or category_match
    
    @RateLimiter().rate_limited(max_requests=60, window=3600)
    def collect_from_feed_2025(self, feed_config: Dict, max_articles: int = 5) -> List[Article]:
        """2025å¹´å¯¾å¿œãƒ•ã‚£ãƒ¼ãƒ‰åé›†"""
        articles = []
        
        try:
            print(f"ğŸ“¡ åé›†ä¸­: {feed_config['source_name']} (ä¿¡é ¼æ€§:{feed_config['reliability']}/5)")
            
            feed = self.parse_rss_safely(feed_config['url'])
            if not feed or not hasattr(feed, 'entries') or len(feed.entries) == 0:
                print(f"  âš ï¸ ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—å¤±æ•—ã¾ãŸã¯ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãªã—")
                return articles
            
            print(f"  ğŸ“„ {len(feed.entries)}å€‹ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’ç™ºè¦‹")
            
            for entry in feed.entries[:max_articles * 3]:  # å¤šã‚ã«å–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿
                try:
                    # æ—¥ä»˜å‡¦ç†
                    pub_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        pub_date = datetime(*entry.updated_parsed[:6])
                    else:
                        pub_date = datetime.now()
                    
                    # æ–°é®®åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆæ›´æ–°é »åº¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰
                    max_age_days = {
                        'hourly': 1,
                        'daily': 3,
                        'weekly': 14,
                        'monthly': 30
                    }.get(feed_config.get('update_freq', 'daily'), 7)
                    
                    if pub_date < datetime.now() - timedelta(days=max_age_days):
                        continue
                    
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
                    title = getattr(entry, 'title', '')
                    content = ""
                    
                    if hasattr(entry, 'summary'):
                        content = entry.summary
                    elif hasattr(entry, 'description'):
                        content = entry.description
                    elif hasattr(entry, 'content'):
                        if isinstance(entry.content, list) and len(entry.content) > 0:
                            content = entry.content[0].get('value', '')
                    
                    # HTMLã‚¿ã‚°å‰Šé™¤
                    content = re.sub(r'<[^>]+>', '', content)
                    content = re.sub(r'\s+', ' ', content).strip()
                    
                    # æœ€ä½æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
                    min_length = 50 if feed_config['lang'] == 'ja' else 80
                    if len(content) < min_length:
                        continue
                    
                    # 2025å¹´å¯¾å¿œAIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if not self.enhance_ai_filtering_2025(content, title, feed_config['category']):
                        continue
                    
                    # è¨˜äº‹ä½œæˆ
                    article_id = f"{feed_config['category']}_{hashlib.md5((feed_config['source_name'] + entry.link).encode()).hexdigest()[:8]}"
                    
                    article = Article(
                        id=article_id,
                        title=title[:200],  # ã‚¿ã‚¤ãƒˆãƒ«é•·åˆ¶é™
                        url=getattr(entry, 'link', ''),
                        source=feed_config['source_name'],
                        source_tier=feed_config['tier'],
                        published_date=pub_date,
                        content=content[:800] + "..." if len(content) > 800 else content,
                        tags=[
                            feed_config['category'], 
                            feed_config['lang'], 
                            'ai_2025',
                            f"reliability_{feed_config['reliability']}",
                            feed_config.get('update_freq', 'daily')
                        ]
                    )
                    
                    articles.append(article)
                    print(f"    âœ… åé›†: {title[:60]}...")
                    
                    # ç›®æ¨™æ•°ã«é”ã—ãŸã‚‰åœæ­¢
                    if len(articles) >= max_articles:
                        break
                    
                except Exception as e:
                    print(f"    âš ï¸ ã‚¨ãƒ³ãƒˆãƒªãƒ¼å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)[:60]}")
                    continue
            
            print(f"  âœ… {feed_config['source_name']}: {len(articles)}è¨˜äº‹åé›†å®Œäº†")
            
        except Exception as e:
            print(f"  âŒ {feed_config['source_name']}åé›†ã‚¨ãƒ©ãƒ¼: {str(e)[:80]}")
        
        return articles
    
    def deduplicate_advanced_2025(self, articles: List[Article]) -> List[Article]:
        """2025å¹´å¯¾å¿œé«˜åº¦é‡è¤‡å‰Šé™¤"""
        unique_articles = []
        seen_urls = set()
        seen_title_hashes = set()
        
        for article in articles:
            # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯
            if article.url in seen_urls:
                continue
            
            # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šé«˜åº¦ï¼‰
            normalized_title = re.sub(r'[^\w\s]', '', article.title.lower())
            normalized_title = re.sub(r'\s+', ' ', normalized_title).strip()
            title_hash = hashlib.md5(normalized_title[:100].encode()).hexdigest()
            
            if title_hash in seen_title_hashes:
                continue
            
            seen_urls.add(article.url)
            seen_title_hashes.add(title_hash)
            unique_articles.append(article)
        
        return unique_articles
    
    def collect_all_2025(self) -> List[Article]:
        """2025å¹´å¯¾å¿œå…¨ã‚½ãƒ¼ã‚¹åé›†"""
        all_articles = []
        
        print("ğŸš€ 2024-2025å¹´æœ€æ–°å¯¾å¿œ åŒ…æ‹¬çš„AIæƒ…å ±åé›†é–‹å§‹")
        print("=" * 70)
        
        # ä¿¡é ¼æ€§é †ã«ã‚½ãƒ¼ãƒˆï¼ˆé«˜ä¿¡é ¼æ€§ã‹ã‚‰å‡¦ç†ï¼‰
        sorted_sources = sorted(
            self.sources.items(), 
            key=lambda x: (x[1]['tier'], -x[1]['reliability'])
        )
        
        category_stats = defaultdict(int)
        lang_stats = defaultdict(int)
        
        for source_key, config in sorted_sources:
            articles = self.collect_from_feed_2025(config)
            all_articles.extend(articles)
            
            # çµ±è¨ˆæ›´æ–°
            category_stats[config['category']] += len(articles)
            lang_stats[config['lang']] += len(articles)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™è€ƒæ…®
            time.sleep(1)
        
        # é«˜åº¦ãªé‡è¤‡å‰Šé™¤
        unique_articles = self.deduplicate_advanced_2025(all_articles)
        
        print(f"\nğŸ“Š 2025å¹´å¯¾å¿œåé›†çµæœ:")
        print(f"  â€¢ ç·è¨˜äº‹æ•°: {len(all_articles)}")
        print(f"  â€¢ é‡è¤‡é™¤å»å¾Œ: {len(unique_articles)}")
        print(f"  â€¢ æ—¥æœ¬èªè¨˜äº‹: {lang_stats['ja']}")
        print(f"  â€¢ è‹±èªè¨˜äº‹: {lang_stats['en']}")
        
        print(f"\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ:")
        for category, count in sorted(category_stats.items()):
            category_name = {
                'research': 'ğŸ“ ç ”ç©¶',
                'community': 'ğŸ‘¥ ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£',
                'tech_media': 'ğŸ“° è‹±èªãƒ¡ãƒ‡ã‚£ã‚¢',
                'tech_media_jp': 'ğŸ‡¯ğŸ‡µ æ—¥æœ¬ãƒ¡ãƒ‡ã‚£ã‚¢',
                'newsletter': 'ğŸ“§ ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼',
                'video': 'ğŸ“º å‹•ç”»',
                'podcast': 'ğŸ™ï¸ ãƒãƒƒãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ',
                'ai_research': 'ğŸ”¬ AIç ”ç©¶'
            }.get(category, category)
            print(f"  â€¢ {category_name}: {count}è¨˜äº‹")
        
        return unique_articles

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œï¼ˆ2025å¹´å¯¾å¿œç‰ˆï¼‰"""
    print("ğŸŒŸ Daily AI News - 2025å¹´æœ€æ–°å®Œå…¨å¯¾å¿œç‰ˆ")
    print("å‚è€ƒè³‡æ–™ã€Œè¦ä»¶è¿½åŠ .txtã€å®Œå…¨å®Ÿè£…")
    print("=" * 80)
    
    settings = Settings()
    
    # 2025å¹´å¯¾å¿œåé›†ã‚·ã‚¹ãƒ†ãƒ 
    collector = Comprehensive2025AICollector()
    articles = collector.collect_all_2025()
    
    if not articles:
        print("âŒ è¨˜äº‹ãŒåé›†ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return False
    
    # 2025å¹´å¯¾å¿œè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
    for article in articles:
        content_lower = (article.title + " " + article.content).lower()
        
        # ä¿¡é ¼æ€§ãƒ™ãƒ¼ã‚¹è©•ä¾¡
        reliability = int([tag for tag in article.tags if tag.startswith('reliability_')][0].split('_')[1])
        base_multiplier = reliability / 5.0  # ä¿¡é ¼æ€§5æ®µéšã‚’ä¿‚æ•°åŒ–
        
        # è¨€èªåˆ¥ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        if 'ja' in article.tags:
            base_eng = 0.6 * base_multiplier
            base_bus = 0.5 * base_multiplier
        else:  # è‹±èªè¨˜äº‹
            base_eng = 0.7 * base_multiplier
            base_bus = 0.6 * base_multiplier
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒœãƒ¼ãƒŠã‚¹ï¼ˆ2025å¹´ç‰ˆï¼‰
        category_bonuses = {
            'research': {'eng': 0.25, 'bus': 0.10},
            'ai_research': {'eng': 0.30, 'bus': 0.15},
            'community': {'eng': 0.15, 'bus': 0.05},
            'tech_media': {'eng': 0.10, 'bus': 0.20},
            'tech_media_jp': {'eng': 0.10, 'bus': 0.15},
            'newsletter': {'eng': 0.20, 'bus': 0.25},
            'video': {'eng': 0.15, 'bus': 0.10},
            'podcast': {'eng': 0.10, 'bus': 0.15}
        }
        
        category = [tag for tag in article.tags if tag in category_bonuses.keys()]
        if category:
            bonuses = category_bonuses[category[0]]
            base_eng += bonuses['eng']
            base_bus += bonuses['bus']
        
        # 2025å¹´æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒœãƒ¼ãƒŠã‚¹
        tech_keywords_2025 = ['rag', 'fine-tuning', 'multimodal', 'agent', 'gpt-4', 'claude', 'gemini', 'llama']
        tech_bonus = min(0.20, sum(0.03 for kw in tech_keywords_2025 if kw in content_lower))
        
        # ãƒ“ã‚¸ãƒã‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒœãƒ¼ãƒŠã‚¹
        biz_keywords_2025 = ['enterprise', 'saas', 'api', 'automation', 'productivity', 'roi', 'deployment']
        biz_bonus = min(0.20, sum(0.03 for kw in biz_keywords_2025 if kw in content_lower))
        
        article.evaluation = {
            "engineer": {"total_score": min(1.0, base_eng + tech_bonus)},
            "business": {"total_score": min(1.0, base_bus + biz_bonus)}
        }
        
        # 2025å¹´å¯¾å¿œãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        has_practical = any(kw in content_lower for kw in ['api', 'tutorial', 'github', 'demo', 'implementation'])
        has_research = any(kw in content_lower for kw in ['paper', 'arxiv', 'research', 'study', 'experiment'])
        
        article.technical = TechnicalMetadata(
            implementation_ready=has_practical,
            code_available='github' in content_lower or 'code' in content_lower,
            reproducibility_score=0.9 if has_research else 0.7 if has_practical else 0.5
        )
        
        # å®Ÿè£…ã‚³ã‚¹ãƒˆæ¨å®š
        if 'research' in article.tags or has_research:
            impl_cost = ImplementationCost.HIGH
        elif 'tutorial' in content_lower or has_practical:
            impl_cost = ImplementationCost.LOW
        else:
            impl_cost = ImplementationCost.MEDIUM
        
        article.business = BusinessMetadata(
            market_size="ã‚°ãƒ­ãƒ¼ãƒãƒ«" if 'en' in article.tags else "æ—¥æœ¬ä¸­å¿ƒ",
            growth_rate=article.evaluation["business"]["total_score"] * 100,
            implementation_cost=impl_cost
        )
    
    # ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
    articles.sort(key=lambda x: (
        x.evaluation["engineer"]["total_score"] + 
        x.evaluation["business"]["total_score"]
    ) / 2, reverse=True)
    
    # ãƒˆãƒƒãƒ—è¨˜äº‹è¡¨ç¤º
    print(f"\nğŸ† ãƒˆãƒƒãƒ—15è¨˜äº‹ï¼ˆ2025å¹´å¯¾å¿œè©•ä¾¡ï¼‰:")
    for i, article in enumerate(articles[:15]):
        eng_score = article.evaluation["engineer"]["total_score"]
        bus_score = article.evaluation["business"]["total_score"]
        combined = (eng_score + bus_score) / 2
        
        # ã‚¢ã‚¤ã‚³ãƒ³è¡¨ç¤º
        lang_icon = "ğŸ‡¯ğŸ‡µ" if 'ja' in article.tags else "ğŸŒ"
        tier_icon = "â­" if article.source_tier == 1 else "âš¡"
        reliability = int([tag for tag in article.tags if tag.startswith('reliability_')][0].split('_')[1])
        reliability_stars = "â˜…" * reliability
        
        print(f"  {i+1:2}. {lang_icon}{tier_icon} {article.title[:50]}...")
        print(f"      ã‚½ãƒ¼ã‚¹: {article.source} ({reliability_stars})")
        print(f"      ã‚¹ã‚³ã‚¢: ç·åˆ {combined:.3f} (ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ {eng_score:.2f} | ãƒ“ã‚¸ãƒã‚¹ {bus_score:.2f})")
        print()
    
    # ã‚µã‚¤ãƒˆç”Ÿæˆ
    try:
        generator = StaticSiteGenerator(settings)
        
        docs_dir = Path("docs")
        docs_dir.mkdir(exist_ok=True)
        
        # ã‚¢ã‚»ãƒƒãƒˆç”Ÿæˆ
        css_content = generator.assets.generate_css()
        (docs_dir / "styles.css").write_text(css_content, encoding='utf-8')
        
        js_content = generator.assets.generate_javascript()
        (docs_dir / "script.js").write_text(js_content, encoding='utf-8')
        
        # HTMLç”Ÿæˆ
        html_generator = generator.html_generator
        processed_articles = html_generator._process_articles(articles, "engineer")
        summary_stats = html_generator._generate_summary_stats(articles)
        
        articles_html = html_generator._render_articles_grid(processed_articles, "engineer")
        filters_html = html_generator._create_interactive_filters(html_generator._extract_filter_options(articles))
        stats_html = html_generator._render_summary_stats(summary_stats)
        
        dashboard_template = html_generator.template_engine.load_template("dashboard.html")
        dashboard_content = html_generator.template_engine.render(dashboard_template, {
            "articles": articles_html,
            "filters": filters_html,
            "summary_stats": stats_html
        })
        
        page_content = html_generator._generate_complete_page(
            dashboard_content,
            title="Daily AI News - 2025å¹´æœ€æ–°å®Œå…¨å¯¾å¿œç‰ˆ",
            description="2024-2025å¹´æœ€æ–°ã®AIæƒ…å ±ã‚’50+ã‚½ãƒ¼ã‚¹ã‹ã‚‰åŒ…æ‹¬åé›†ãƒ»å¤šå±¤è©•ä¾¡",
            persona="engineer"
        )
        
        index_file = docs_dir / "index.html"
        index_file.write_text(page_content, encoding='utf-8')
        
        print(f"\nâœ… 2025å¹´å¯¾å¿œAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆç”Ÿæˆå®Œäº†!")
        print(f"ğŸ“‚ å ´æ‰€: {index_file.absolute()}")
        print(f"ğŸŒŸ æœ€æ–°ã®2024-2025å¹´AIæŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å®Œå…¨ç¶²ç¾…ï¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚µã‚¤ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    
    if success:
        print("\nğŸ‰ 2025å¹´æœ€æ–°å¯¾å¿œAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆãŒå®Œæˆã—ã¾ã—ãŸï¼")
        print("ğŸŒŸ ç‰¹å¾´:")
        print("  ğŸ“Š ä¿¡é ¼æ€§ãƒ™ãƒ¼ã‚¹è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆâ˜…1-5æ®µéšï¼‰")
        print("  ğŸ¯ 2025å¹´æœ€æ–°AIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œ")
        print("  ğŸŒ 50+ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®åŒ…æ‹¬åé›†")
        print("  ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªãƒ»è‹±èªå®Œå…¨å¯¾å¿œ")
        print("  ğŸ“¡ arXivã€Redditã€Substackç­‰ã®é«˜ä¿¡é ¼æ€§ã‚½ãƒ¼ã‚¹")
        print("  ğŸ” RAGã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã€AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç­‰æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰")
        print("  âš¡ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œå…¨å¯¾å¿œ")
    else:
        print("\nâš ï¸ å®Ÿè¡Œã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ")
    
    input("\nEnterã‚’æŠ¼ã—ã¦çµ‚äº†...")