#!/usr/bin/env python3
"""
Xè¨˜äº‹åé›†å™¨ - Google Spreadsheetsã‹ã‚‰ã®æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿å–å¾—
"""

import re
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import hashlib
from pathlib import Path
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src"))

try:
    from src.models.article import Article, TechnicalMetadata, BusinessMetadata, ImplementationCost
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®æœ€å°ãƒ¢ãƒ‡ãƒ«
    class Article:
        def __init__(self, **kwargs):
            for k, v in kwargs.items():
                setattr(self, k, v)

class XSourceCollector:
    """Google Spreadsheetsã‹ã‚‰xè¨˜äº‹ã‚’å–å¾—"""
    
    def __init__(self):
        self.spreadsheet_url = "https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/edit?gid=0#gid=0"
        self.csv_export_url = "https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0&access_token=&headers=false"
    
    def parse_x_articles(self) -> List[Article]:
        """Google Spreadsheetsã‹ã‚‰Xè¨˜äº‹ã‚’å–å¾—ãƒ»è§£æ"""
        articles = []
        
        try:
            print("ğŸ“¡ Xè¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
            print(f"ğŸ”— URL: {self.csv_export_url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/csv,text/plain,*/*',
                'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8'
            }
            
            # è¤‡æ•°ã®CSVå–å¾—æ–¹æ³•ã‚’è©¦è¡Œ
            csv_urls = [
                self.csv_export_url,
                f"https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/gviz/tq?tqx=out:csv&gid=0",
                f"https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv"
            ]
            
            response = None
            for csv_url in csv_urls:
                try:
                    print(f"ğŸ”„ è©¦è¡Œä¸­: {csv_url}")
                    response = requests.get(csv_url, headers=headers, timeout=15, allow_redirects=True)
                    
                    # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
                    print(f"    ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
                    print(f"    Content-Length: {len(response.content)} bytes")
                    print(f"    Content-Type: {response.headers.get('content-type', 'N/A')}")
                    
                    if response.status_code == 200 and len(response.text) > 10:
                        print("âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ")
                        
                        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æƒ…å ±
                        encoding = response.encoding or 'utf-8'
                        print(f"    æ¤œå‡ºã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding}")
                        
                        # å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                        preview = response.text[:200].replace('\n', '\\n')
                        print(f"    å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {preview}...")
                        
                        # æ–‡å­—åŒ–ã‘æ¤œå‡ºã¨ä¿®æ­£ï¼ˆã‚ˆã‚Šç©æ¥µçš„ï¼‰
                        if 'ï¿½' in response.text or 'Ãƒ' in response.text or 'Ã£' in response.text[:100]:
                            print("    âš ï¸ æ–‡å­—åŒ–ã‘ã‚’æ¤œå‡ºã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä¿®æ­£ä¸­...")
                            encodings_to_try = ['utf-8', 'shift-jis', 'euc-jp', 'cp932', 'iso-2022-jp']
                            for enc in encodings_to_try:
                                try:
                                    corrected_content = response.content.decode(enc, errors='ignore')
                                    # æ—¥æœ¬èªæ–‡å­—ãŒæ­£ã—ãè¡¨ç¤ºã•ã‚Œã‚‹ã‹ãƒ†ã‚¹ãƒˆ
                                    if 'AI' in corrected_content and ('ã®' in corrected_content or 'ã€' in corrected_content):
                                        print(f"    âœ… {enc}ã§ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ãŒæˆåŠŸ")
                                        # responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å†…å®¹ã‚’æ›´æ–°
                                        response.encoding = 'utf-8'
                                        response._content = corrected_content.encode('utf-8')
                                        break
                                except Exception as decode_error:
                                    print(f"    âš ï¸ {enc}ãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—: {str(decode_error)[:30]}")
                                    continue
                        
                        break
                    else:
                        print(f"    âŒ ç„¡åŠ¹ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text[:100]}...")
                        
                except Exception as e:
                    print(f"  âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue
            
            if not response or response.status_code != 200 or len(response.text) <= 10:
                print(f"âŒ å…¨ã¦ã®æ–¹æ³•ã§Spreadsheetså–å¾—ã«å¤±æ•—")
                
                # ä»£æ›¿æ‰‹æ®µã®è©³ç´°ãªèª¬æ˜
                print("\nğŸ’¡ æ‰‹å‹•ã§CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„:")
                print("  1. ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’é–‹ã")
                print("  2. ã€Œãƒ•ã‚¡ã‚¤ãƒ«ã€â†’ã€Œãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€â†’ã€Œã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šå€¤(.csv)ã€")
                print(f"  3. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä»¥ä¸‹ã«ä¿å­˜:")
                print(f"     {project_root / 'å‚è€ƒ' / 'x_articles.csv'}")
                print("  4. å†å®Ÿè¡Œã—ã¦ãã ã•ã„")
                
                return articles
            
            csv_content = response.text
            lines = csv_content.strip().split('\n')
            
            print(f"ğŸ“„ {len(lines)}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
            
            print(f"  ğŸ“‹ å‡¦ç†é–‹å§‹: ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦è§£æ...")
            processed_count = 0
            
            for i, line in enumerate(lines[1:], 1):  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                try:
                    # ç©ºè¡Œã‚„çŸ­ã™ãã‚‹è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if len(line.strip()) < 10:
                        continue
                        
                    # CSVè§£æï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã‚‚å¯¾å¿œï¼‰
                    if '|' in line:
                        parts = [p.strip().strip('"') for p in line.split('|')]
                    else:
                        # æ¨™æº–çš„ãªCSVè§£æ
                        import csv
                        import io
                        csv_reader = csv.reader(io.StringIO(line))
                        try:
                            parts = next(csv_reader)
                        except:
                            continue
                    
                    if len(parts) < 3:  # æœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
                        continue
                    
                    processed_count += 1
                    if processed_count % 50 == 0:
                        print(f"    ğŸ“Š å‡¦ç†æ¸ˆã¿: {processed_count}è¡Œ")
                    
                    created_at_str = parts[0].strip()
                    username = parts[1].strip().replace('@', '')
                    post_content = parts[2].strip()
                    first_link = parts[3].strip() if len(parts) > 3 else ""
                    tweet_link = parts[4].strip() if len(parts) > 4 else ""
                    
                    # Xè¨˜äº‹ç‰¹æœ‰ã®æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†
                    import html
                    post_content = html.unescape(post_content)
                    # UTF-8ã¨ã—ã¦æ­£ã—ãå‡¦ç†
                    try:
                        if isinstance(post_content, bytes):
                            post_content = post_content.decode('utf-8', errors='ignore')
                    except:
                        pass
                    
                    # æ—¥ä»˜è§£æï¼ˆtimezoneå¯¾å¿œï¼‰
                    try:
                        if 'T' in created_at_str and created_at_str:
                            # ISOå½¢å¼ã®æ—¥ä»˜å‡¦ç†
                            if created_at_str.endswith('Z'):
                                created_at_str = created_at_str[:-1] + '+00:00'
                            pub_date = datetime.fromisoformat(created_at_str)
                            # timezone-naiveã«å¤‰æ›
                            if pub_date.tzinfo is not None:
                                pub_date = pub_date.replace(tzinfo=None)
                        else:
                            pub_date = datetime.now()
                    except Exception as e:
                        print(f"    âš ï¸ æ—¥ä»˜è§£æã‚¨ãƒ©ãƒ¼: {created_at_str[:20]} - ç¾åœ¨æ—¥ä»˜ã‚’ä½¿ç”¨")
                        pub_date = datetime.now()
                    
                    # 7æ—¥ä»¥å†…ã®è¨˜äº‹ã®ã¿ï¼ˆXè¨˜äº‹ã¯é€Ÿå ±æ€§é‡è¦–ï¼‰
                    if pub_date < datetime.now() - timedelta(days=7):
                        continue
                    
                    # AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if not self.is_ai_related(post_content):
                        continue
                    
                    # è¨˜äº‹ä½œæˆ
                    article_id = f"x_{hashlib.md5((username + post_content).encode()).hexdigest()[:8]}"
                    
                    # URLå‡¦ç† - å¤–éƒ¨ãƒªãƒ³ã‚¯ãŒãªã„å ´åˆã¯ãƒ„ã‚¤ãƒ¼ãƒˆãƒªãƒ³ã‚¯ã‚’ä½¿ç”¨
                    article_url = first_link if first_link else tweet_link
                    if not article_url:
                        # ãƒ„ã‚¤ãƒ¼ãƒˆãƒªãƒ³ã‚¯ã‚‚ãªã„å ´åˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‹ã‚‰æ¨æ¸¬
                        article_url = f"https://twitter.com/{username}"
                    
                    article = Article(
                        id=article_id,
                        title=self.extract_title_from_post(post_content),
                        url=article_url,
                        source=f"X(@{username})",
                        source_tier=2,  # Xè¨˜äº‹ã¯ä¿¡é ¼æ€§tier 2
                        published_date=pub_date,
                        content=post_content[:300] + "..." if len(post_content) > 300 else post_content,
                        tags=['x_post', 'community', 'ai_2025']
                    )
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                    article.technical = TechnicalMetadata(
                        implementation_ready='github' in post_content.lower() or 'code' in post_content.lower(),
                        code_available='github.com' in post_content.lower(),
                        reproducibility_score=0.6  # Xè¨˜äº‹ã¯ä¸­ç¨‹åº¦ã®å†ç¾æ€§
                    )
                    
                    article.business = BusinessMetadata(
                        market_size="ã‚°ãƒ­ãƒ¼ãƒãƒ«",
                        growth_rate=70,  # Xè¨˜äº‹ã¯é€Ÿå ±æ€§ãŒé«˜ã„
                        implementation_cost=ImplementationCost.LOW
                    )
                    
                    # è©•ä¾¡ã‚¹ã‚³ã‚¢
                    ai_score = self.calculate_ai_relevance_score(post_content)
                    article.evaluation = {
                        "engineer": {"total_score": min(1.0, ai_score + 0.1)},  # Xè¨˜äº‹ã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘
                        "business": {"total_score": min(1.0, ai_score * 0.8)}  # ãƒ“ã‚¸ãƒã‚¹å‘ã‘ã¯å°‘ã—ä½ã‚
                    }
                    
                    articles.append(article)
                    print(f"  âœ… åé›†: @{username} - {self.extract_title_from_post(post_content)[:40]}...")
                    
                except Exception as e:
                    print(f"  âš ï¸ è¡Œ{i}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}")
                    continue
            
            print(f"âœ… Xè¨˜äº‹åé›†å®Œäº†: {len(articles)}è¨˜äº‹")
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
            if len(articles) == 0:
                print(f"  ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
                print(f"    â€¢ å‡¦ç†æ¸ˆã¿è¡Œæ•°: {processed_count}")
                print(f"    â€¢ AIé–¢é€£ã§ãƒ•ã‚£ãƒ«ã‚¿ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                print(f"    â€¢ æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿: 7æ—¥ä»¥å†…ã®è¨˜äº‹ã®ã¿")
                
                # æœ€åˆã®5è¡Œã‚’ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
                if len(lines) > 5:
                    print(f"  ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3è¡Œï¼‰:")
                    for i, line in enumerate(lines[1:4]):
                        parts = line.split('|')
                        if len(parts) >= 3:
                            print(f"    {i+1}. ãƒ¦ãƒ¼ã‚¶ãƒ¼: {parts[1][:20]}... å†…å®¹: {parts[2][:30]}...")
                        else:
                            print(f"    {i+1}. è§£æä¸å¯: {line[:50]}...")
            
        except Exception as e:
            print(f"âŒ Xè¨˜äº‹åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return articles
    
    def is_ai_related(self, content: str) -> bool:
        """AIé–¢é€£åˆ¤å®šï¼ˆ2025å¹´å¯¾å¿œï¼‰"""
        text = content.lower()
        
        ai_keywords = [
            # åŸºæœ¬AIç”¨èª
            'ai', 'artificial intelligence', 'machine learning', 'ml', 'deep learning',
            'neural network', 'neural net', 'computer vision', 'nlp', 'natural language processing',
            
            # å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«
            'llm', 'large language model', 'chatgpt', 'gpt-4', 'gpt-5', 'gpt4', 'gpt5',
            'claude', 'gemini', 'bard', 'llama', 'mistral', 'openai', 'anthropic',
            
            # ç”ŸæˆAI
            'generative ai', 'gen ai', 'stable diffusion', 'midjourney', 'dall-e', 'sora',
            'diffusion', 'text-to-image', 'image generation', 'text generation',
            
            # 2025å¹´æŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰
            'rag', 'retrieval augmented generation', 'fine-tuning', 'multimodal',
            'ai agent', 'ai agents', 'transformer', 'attention', 'foundation model',
            'prompt engineering', 'in-context learning', 'few-shot', 'zero-shot',
            
            # AIä¼æ¥­ãƒ»ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆ
            'microsoft copilot', 'github copilot', 'huggingface', 'nvidia', 'meta ai',
            'google ai', 'deepmind', 'pytorch', 'tensorflow', 'langchain',
            
            # æ—¥æœ¬èªAIç”¨èª
            'äººå·¥çŸ¥èƒ½', 'AI', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'ãƒãƒ£ãƒƒãƒˆGPT', 
            'LLM', 'ç”ŸæˆAI', 'AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ', 'ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°', 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ'
        ]
        
        return any(keyword in text for keyword in ai_keywords)
    
    def extract_title_from_post(self, content: str) -> str:
        """æŠ•ç¨¿å†…å®¹ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
        # æœ€åˆã®æ–‡ã¾ãŸã¯50æ–‡å­—ä»¥å†…
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', content)
        if sentences and len(sentences[0]) <= 100:
            return sentences[0].strip()
        return content[:50] + "..." if len(content) > 50 else content
    
    def calculate_ai_relevance_score(self, content: str) -> float:
        """AIé–¢é€£åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        text = content.lower()
        
        # é«˜ä¾¡å€¤ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        high_value_keywords = ['gpt-4', 'claude', 'gemini', 'rag', 'multimodal', 'ai agent']
        score = 0.5
        
        for keyword in high_value_keywords:
            if keyword in text:
                score += 0.1
        
        # ãƒªãƒ³ã‚¯ãŒã‚ã‚‹å ´åˆã¯ãƒœãƒ¼ãƒŠã‚¹
        if 'http' in content:
            score += 0.05
        
        return min(1.0, score)

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    collector = XSourceCollector()
    articles = collector.parse_x_articles()
    
    print(f"\nğŸ“Š Xè¨˜äº‹åé›†çµæœ:")
    print(f"  â€¢ ç·è¨˜äº‹æ•°: {len(articles)}")
    
    if articles:
        print(f"\nğŸ† ãƒˆãƒƒãƒ—5è¨˜äº‹:")
        for i, article in enumerate(articles[:5]):
            print(f"  {i+1}. {article.source} - {article.title}")
            print(f"     ã‚¹ã‚³ã‚¢: {article.evaluation['engineer']['total_score']:.2f}")

if __name__ == "__main__":
    main()