#!/usr/bin/env python3
"""
çµ±åˆç‰ˆè¨˜äº‹åé›†ã‚·ã‚¹ãƒ†ãƒ 
Xè¨˜äº‹ + RSSè¨˜äº‹ã®ä¸¡æ–¹ã‚’çµ±åˆè¡¨ç¤º
"""

import requests
import json
import csv
try:
    import feedparser  # optional; skip RSS if unavailable
except Exception:
    feedparser = None
from io import StringIO
from datetime import datetime, timedelta
from pathlib import Path
import hashlib
import re
import html
from urllib.parse import urljoin, urlparse
import time
from evaluation_system import MultiLayerEvaluator


class IntegratedCollector:
    """Xè¨˜äº‹ã¨RSSè¨˜äº‹ã‚’çµ±åˆåé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.spreadsheet_url = "https://docs.google.com/spreadsheets/d/1uuLKCLIJw--a1vCcO6UGxSpBiLTtN8uGl2cdMb6wcfg/export?format=csv&gid=0"
        self.project_root = Path(__file__).parent
        self.docs_path = self.project_root / "docs"
        self.evaluator = MultiLayerEvaluator()
        
        # RSS ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®š
        self.rss_feeds = {
            "MIT Technology Review AI": {
                "url": "https://www.technologyreview.com/topic/artificial-intelligence/feed/",
                "tier": 1
            },
            "Reddit MachineLearning": {
                "url": "https://www.reddit.com/r/MachineLearning/.rss",
                "tier": 1
            },
            "Reddit LocalLLaMA": {
                "url": "https://www.reddit.com/r/LocalLLaMA/.rss",
                "tier": 1
            },
            "VentureBeat AI": {
                "url": "https://venturebeat.com/ai/feed/",
                "tier": 1
            },
            "TechCrunch AI": {
                "url": "https://techcrunch.com/category/artificial-intelligence/feed/",
                "tier": 1
            },
            "Hacker News AI": {
                "url": "https://hnrss.org/newest?q=AI+OR+machine+learning+OR+deep+learning",
                "tier": 2
            }
        }
    
    def translate_and_summarize(self, text, title=""):
        """è‹±èªè¨˜äº‹ã‚’æ—¥æœ¬èªè¦ç´„ã«å¤‰æ›"""
        if not text or not text.strip():
            return text
        
        # æ—¢ã«æ—¥æœ¬èªã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if self.is_japanese(text):
            return text[:200] + "..." if len(text) > 200 else text
        
        try:
            # ç°¡æ˜“çš„ãªè¦ç´„ãƒ»ç¿»è¨³å‡¦ç†
            # å®Ÿéš›ã®AIè¦ç´„ã®ä»£ã‚ã‚Šã«ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®è¦ç´„ã‚’ä½œæˆ
            summary_jp = self.create_japanese_summary(text, title)
            return summary_jp
            
        except Exception as e:
            print(f"Translation error: {str(e)}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è‹±èªã‚’ãã®ã¾ã¾ä½¿ç”¨
            return text[:200] + "..." if len(text) > 200 else text
    
    def is_japanese(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆãŒæ—¥æœ¬èªã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        japanese_chars = sum(1 for c in text if '\u3040' <= c <= '\u309F' or '\u30A0' <= c <= '\u30FF' or '\u4E00' <= c <= '\u9FAF')
        return japanese_chars > len(text) * 0.1  # 10%ä»¥ä¸Šæ—¥æœ¬èªæ–‡å­—ãŒã‚ã‚Œã°æ—¥æœ¬èªã¨åˆ¤å®š
    
    def create_japanese_summary(self, english_text, title=""):
        """è‹±èªè¨˜äº‹ã®æ—¥æœ¬èªè¦ç´„ã‚’ä½œæˆ"""
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ç¿»è¨³ãƒ»è¦ç´„
        key_translations = {
            # AIé–¢é€£ç”¨èª
            "artificial intelligence": "äººå·¥çŸ¥èƒ½",
            "machine learning": "æ©Ÿæ¢°å­¦ç¿’", 
            "deep learning": "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°",
            "neural network": "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
            "transformer": "ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼",
            "language model": "è¨€èªãƒ¢ãƒ‡ãƒ«",
            "chatgpt": "ChatGPT",
            "gpt-4": "GPT-4",
            "gpt-5": "GPT-5", 
            "claude": "Claude",
            "openai": "OpenAI",
            "anthropic": "Anthropic",
            "google": "Google",
            "microsoft": "Microsoft",
            "meta": "Meta",
            "facebook": "Facebook",
            "research": "ç ”ç©¶",
            "model": "ãƒ¢ãƒ‡ãƒ«",
            "algorithm": "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ",
            "data": "ãƒ‡ãƒ¼ã‚¿",
            "training": "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°",
            "inference": "æ¨è«–",
            "performance": "æ€§èƒ½",
            "benchmark": "ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯",
            "breakthrough": "ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼",
            "innovation": "ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³",
            "technology": "æŠ€è¡“",
            "development": "é–‹ç™º",
            "release": "ãƒªãƒªãƒ¼ã‚¹",
            "announcement": "ç™ºè¡¨",
            "improvement": "æ”¹å–„",
            "enhancement": "æ©Ÿèƒ½å¼·åŒ–",
            "new": "æ–°ã—ã„",
            "latest": "æœ€æ–°",
            "advanced": "é«˜åº¦ãª",
            "powerful": "å¼·åŠ›ãª",
            "efficient": "åŠ¹ç‡çš„ãª",
            "accuracy": "ç²¾åº¦",
            "capability": "èƒ½åŠ›",
            "feature": "æ©Ÿèƒ½",
            "tool": "ãƒ„ãƒ¼ãƒ«",
            "platform": "ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ",
            "system": "ã‚·ã‚¹ãƒ†ãƒ ",
            "framework": "ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯",
            "library": "ãƒ©ã‚¤ãƒ–ãƒ©ãƒª",
            "api": "API",
            "application": "ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³",
            "software": "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢",
            "hardware": "ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢",
            "chip": "ãƒãƒƒãƒ—",
            "processor": "ãƒ—ãƒ­ã‚»ãƒƒã‚µ",
            "gpu": "GPU",
            "computing": "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°",
            "cloud": "ã‚¯ãƒ©ã‚¦ãƒ‰",
            "edge": "ã‚¨ãƒƒã‚¸",
            "mobile": "ãƒ¢ãƒã‚¤ãƒ«",
            "web": "ã‚¦ã‚§ãƒ–",
            "internet": "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ",
            "startup": "ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—",
            "company": "ä¼æ¥­",
            "business": "ãƒ“ã‚¸ãƒã‚¹",
            "industry": "æ¥­ç•Œ",
            "market": "å¸‚å ´",
            "investment": "æŠ•è³‡",
            "funding": "è³‡é‡‘èª¿é”",
            "revenue": "åç›Š",
            "growth": "æˆé•·",
            "user": "ãƒ¦ãƒ¼ã‚¶ãƒ¼",
            "customer": "é¡§å®¢",
            "product": "è£½å“",
            "service": "ã‚µãƒ¼ãƒ“ã‚¹",
            "solution": "ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³"
        }
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’å°æ–‡å­—ã«ã—ã¦å‡¦ç†
        text_lower = english_text.lower()
        
        # é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        found_keywords = []
        for eng, jp in key_translations.items():
            if eng in text_lower:
                found_keywords.append(jp)
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã®è¦ç´„
        title_summary = ""
        if title:
            title_lower = title.lower()
            for eng, jp in key_translations.items():
                title_lower = title_lower.replace(eng, jp)
            title_summary = title_lower
        
        # è¦ç´„æ–‡ã‚’ç”Ÿæˆ
        if found_keywords:
            summary = f"{title_summary}ã«é–¢ã™ã‚‹è¨˜äº‹ã€‚" if title_summary else ""
            
            # AIé–¢é€£ã®è©±é¡Œã‚’ç‰¹å®š
            if any(kw in found_keywords for kw in ["äººå·¥çŸ¥èƒ½", "æ©Ÿæ¢°å­¦ç¿’", "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°", "ChatGPT", "GPT-4", "GPT-5", "Claude"]):
                summary += "AIæŠ€è¡“ã®"
                
                if any(kw in found_keywords for kw in ["æ–°ã—ã„", "æœ€æ–°", "ãƒªãƒªãƒ¼ã‚¹", "ç™ºè¡¨"]):
                    summary += "æœ€æ–°å‹•å‘ã‚„æ–°æ©Ÿèƒ½ã«ã¤ã„ã¦å ±å‘Šã€‚"
                elif any(kw in found_keywords for kw in ["ç ”ç©¶", "ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼", "ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³"]):
                    summary += "ç ”ç©¶æˆæœã‚„æŠ€è¡“é©æ–°ã«ã¤ã„ã¦è§£èª¬ã€‚"
                elif any(kw in found_keywords for kw in ["æ€§èƒ½", "æ”¹å–„", "æ©Ÿèƒ½å¼·åŒ–"]):
                    summary += "æ€§èƒ½å‘ä¸Šã‚„æ”¹å–„ã«é–¢ã™ã‚‹å†…å®¹ã€‚"
                elif any(kw in found_keywords for kw in ["ãƒ“ã‚¸ãƒã‚¹", "ä¼æ¥­", "å¸‚å ´", "æŠ•è³‡"]):
                    summary += "ãƒ“ã‚¸ãƒã‚¹é¢ã§ã®å‹•ãã‚„å¸‚å ´å‹•å‘ã«ã¤ã„ã¦ã€‚"
                else:
                    summary += "é–‹ç™ºã‚„å¿œç”¨ã«é–¢ã™ã‚‹æƒ…å ±ã€‚"
            
            # ä¼æ¥­åãŒå«ã¾ã‚Œã‚‹å ´åˆ
            companies = [kw for kw in found_keywords if kw in ["OpenAI", "Anthropic", "Google", "Microsoft", "Meta"]]
            if companies:
                summary += f" {', '.join(companies)}ãŒé–¢é€£ã™ã‚‹"
                
                if "ç™ºè¡¨" in found_keywords or "ãƒªãƒªãƒ¼ã‚¹" in found_keywords:
                    summary += "æ–°ã—ã„ç™ºè¡¨ã‚„ãƒªãƒªãƒ¼ã‚¹ã«ã¤ã„ã¦ã€‚"
                elif "ç ”ç©¶" in found_keywords:
                    summary += "ç ”ç©¶é–‹ç™ºã®å–ã‚Šçµ„ã¿ã«ã¤ã„ã¦ã€‚"
                else:
                    summary += "å‹•å‘ã«ã¤ã„ã¦ã€‚"
            
            return summary
        else:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æ±ç”¨çš„ãªè¦ç´„
            return f"AIãƒ»æŠ€è¡“åˆ†é‡ã«é–¢ã™ã‚‹{title_summary}ã®è¨˜äº‹ã€‚æœ€æ–°ã®å‹•å‘ã‚„æŠ€è¡“é–‹ç™ºã«ã¤ã„ã¦ã€‚"
    
    def collect_x_articles(self):
        """Xè¨˜äº‹ã‚’åé›†"""
        print("Collecting X articles...")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(self.spreadsheet_url, headers=headers, timeout=15)
            
            if response.status_code != 200:
                print(f"X articles fetch failed: {response.status_code}")
                return []
            
            # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†
            try:
                csv_text = response.content.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    csv_text = response.content.decode('iso-8859-1')
                except UnicodeDecodeError:
                    csv_text = response.content.decode('shift-jis', errors='ignore')
            
            # CSVã‚’è§£æ
            reader = csv.reader(StringIO(csv_text))
            headers = next(reader)  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            
            x_articles = []
            processed_count = 0
            
            for row in reader:
                if len(row) < 5:
                    continue
                    
                processed_count += 1
                
                # ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                created_at = row[0].strip()
                username = row[1].strip().replace('@', '')
                content = row[2].strip()
                first_link = row[3].strip() if len(row) > 3 else ""
                tweet_link = row[4].strip() if len(row) > 4 else ""
                
                # HTML ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                content = html.unescape(content)
                username = html.unescape(username)
                
                # URLãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                article_url = None
                if first_link and self.is_valid_url(first_link):
                    article_url = first_link
                elif tweet_link and self.is_valid_url(tweet_link):
                    article_url = tweet_link
                else:
                    article_url = f"https://twitter.com/{username}"
                
                # AIé–¢é€£ãƒã‚§ãƒƒã‚¯
                if not self.is_ai_related(content):
                    continue
                
                # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                article = {
                    'id': f"x_{hashlib.md5(f'{username}_{content}'.encode()).hexdigest()[:8]}",
                    'title': self.extract_title(content),
                    'url': article_url,
                    'source': f'X(@{username})',
                    'source_tier': 2,
                    'published_date': self.parse_date(created_at),
                    'content': content[:200] + "..." if len(content) > 200 else content,
                    'tags': ['x_post', 'ai_2025', 'community']
                }
                
                # å¤šå±¤è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã§è©•ä¾¡
                engineer_eval = self.evaluator.evaluate_article(article, 'engineer')
                business_eval = self.evaluator.evaluate_article(article, 'business')
                
                article['evaluation'] = {
                    'engineer': engineer_eval,
                    'business': business_eval
                }
                article['total_score'] = engineer_eval['total_score']  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘
                
                x_articles.append(article)
                
                if len(x_articles) >= 30:  # Xè¨˜äº‹ã¯30ä»¶ã¾ã§
                    break
            
            print(f"X articles collected: {len(x_articles)}")
            # Fallback to local CSV if Google Sheet returns nothing
            if len(x_articles) == 0:
                print("âš ï¸ No X articles from Google Sheet. Trying local CSV fallbackâ€¦")
                fallback = self.collect_x_from_local_csv()
                if fallback:
                    print(f"âœ… Fallback X articles: {len(fallback)}")
                    return fallback
            return x_articles
        
        except Exception as e:
            print(f"X articles collection error: {str(e)}")
            # Fallback on exception as well
            fallback = self.collect_x_from_local_csv()
            if fallback:
                print(f"âœ… Fallback X articles after error: {len(fallback)}")
                return fallback
            return []

    def collect_x_from_local_csv(self):
        """å‚è€ƒ/x_articles.csv ã¾ãŸã¯ x_articles_backup.csv ã‹ã‚‰Xè¨˜äº‹ã‚’èª­ã¿è¾¼ã‚€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        try:
            ref_dir = self.project_root / 'å‚è€ƒ'
            primary = ref_dir / 'x_articles.csv'
            backup = ref_dir / 'x_articles_backup.csv'
            csv_path = None
            if primary.exists() and primary.stat().st_size > 0:
                csv_path = primary
            elif backup.exists() and backup.stat().st_size > 0:
                csv_path = backup
            else:
                print("âŒ No local CSV found for X articles fallback")
                return []

            print(f"ğŸ“‚ Reading X articles from local CSV: {csv_path}")
            rows = []
            with open(csv_path, 'r', encoding='utf-8') as f:
                sample = f.read(2048)
                f.seek(0)
                # Determine if file has headers; use DictReader if so
                has_header = any(h in sample for h in ['created_at', 'username', 'tweet_url', 'content'])
                if has_header:
                    reader = csv.DictReader(f)
                    for r in reader:
                        rows.append([
                            r.get('created_at',''),
                            r.get('username',''),
                            r.get('content',''),
                            r.get('link',''),
                            r.get('tweet_url','')
                        ])
                else:
                    reader = csv.reader(f)
                    for r in reader:
                        rows.append(r)

            # Skip header if present in first row
            if rows and isinstance(rows[0][0], str) and rows[0][0].lower().startswith('created'):
                rows = rows[1:]

            x_articles = []
            for row in rows:
                if not row or len(row) < 3:
                    continue
                created_at = (row[0] or '').strip()
                username = (row[1] or '').strip().replace('@','')
                content = (row[2] or '').strip()
                first_link = (row[3] or '').strip() if len(row) > 3 else ''
                tweet_link = (row[4] or '').strip() if len(row) > 4 else ''

                if not content or not username:
                    continue

                # AI-related filter
                if not self.is_ai_related(content):
                    continue

                article_url = first_link or tweet_link or (f"https://twitter.com/{username}")
                article = {
                    'id': f"x_{hashlib.md5(f'{username}_{content}'.encode()).hexdigest()[:8]}",
                    'title': self.extract_title(content),
                    'url': article_url,
                    'source': f'X(@{username})',
                    'source_tier': 2,
                    'published_date': self.parse_date(created_at) if created_at else datetime.now().strftime('%Y-%m-%d'),
                    'content': content[:200] + "..." if len(content) > 200 else content,
                    'tags': ['x_post', 'ai_2025', 'community']
                }

                engineer_eval = self.evaluator.evaluate_article(article, 'engineer')
                business_eval = self.evaluator.evaluate_article(article, 'business')
                article['evaluation'] = { 'engineer': engineer_eval, 'business': business_eval }
                article['total_score'] = engineer_eval['total_score']

                x_articles.append(article)
                if len(x_articles) >= 30:
                    break

            print(f"Local CSV X articles collected: {len(x_articles)}")
            return x_articles
        except Exception as e:
            print(f"Fallback CSV read error: {e}")
            return []
    
    def collect_rss_articles(self):
        """RSSè¨˜äº‹ã‚’åé›†"""
        print("Collecting RSS articles...")
        if feedparser is None:
            print("feedparser ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€RSSåé›†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return []
        
        rss_articles = []
        cutoff_date = datetime.utcnow() - timedelta(hours=48)  # 48æ™‚é–“ä»¥å†…ã®ã¿ï¼ˆnaive UTCåŸºæº–ï¼‰
        
        for source_name, feed_config in self.rss_feeds.items():
            try:
                print(f"Fetching from {source_name}...")
                
                # ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
                feed = feedparser.parse(feed_config["url"])
                
                if not hasattr(feed, 'entries') or not feed.entries:
                    print(f"No entries found for {source_name}")
                    continue
                
                for entry in feed.entries[:10]:  # å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰æœ€å¤§10è¨˜äº‹
                    try:
                        # æ—¥ä»˜ãƒã‚§ãƒƒã‚¯
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            pub_date = datetime(*entry.published_parsed[:6])
                            if pub_date < cutoff_date:
                                continue
                            date_str = pub_date.strftime('%Y-%m-%d')
                        else:
                            date_str = datetime.now().strftime('%Y-%m-%d')
                        
                        # ã‚¿ã‚¤ãƒˆãƒ«ã¨å†…å®¹ã‚’å–å¾—
                        title = getattr(entry, 'title', 'No Title')
                        url = getattr(entry, 'link', '')
                        
                        # å†…å®¹ã‚’æŠ½å‡º
                        content = ''
                        if hasattr(entry, 'summary'):
                            content = entry.summary
                        elif hasattr(entry, 'description'):
                            content = entry.description
                        else:
                            content = title
                        
                        # HTMLã‚¿ã‚°ã‚’é™¤å»
                        content = re.sub(r'<[^>]+>', '', content)
                        content = html.unescape(content).strip()
                        
                        # AIé–¢é€£ãƒã‚§ãƒƒã‚¯
                        if not self.is_ai_related(f"{title} {content}"):
                            continue
                        
                        # æ—¥æœ¬èªè¦ç´„ã‚’ç”Ÿæˆ
                        japanese_summary = self.translate_and_summarize(content, title)
                        
                        # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                        article = {
                            'id': f"rss_{hashlib.md5(f'{source_name}_{title}'.encode()).hexdigest()[:8]}",
                            'title': title,
                            'url': url,
                            'source': source_name,
                            'source_tier': feed_config["tier"],
                            'published_date': date_str,
                            'content': japanese_summary,
                            'original_content': content[:200] + "..." if len(content) > 200 else content,
                            'tags': ['rss_feed', 'ai_2025', 'tech_media']
                        }
                        
                        # å¤šå±¤è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã§è©•ä¾¡
                        engineer_eval = self.evaluator.evaluate_article(article, 'engineer')
                        business_eval = self.evaluator.evaluate_article(article, 'business')
                        
                        article['evaluation'] = {
                            'engineer': engineer_eval,
                            'business': business_eval
                        }
                        article['total_score'] = engineer_eval['total_score']  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘
                        
                        rss_articles.append(article)
                        
                    except Exception as entry_error:
                        print(f"Error processing entry from {source_name}: {str(entry_error)}")
                        continue
                        
            except Exception as feed_error:
                print(f"Error fetching {source_name}: {str(feed_error)}")
                continue
        
        print(f"RSS articles collected: {len(rss_articles)}")
        return rss_articles
    
    def is_valid_url(self, url):
        """URLã®æœ‰åŠ¹æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        if not url:
            return False
        
        url_pattern = re.compile(
            r'^https?://'
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'
            r'localhost|'
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'
            r'(?::\d+)?'
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)
        
        return bool(url_pattern.match(url))
    
    def is_ai_related(self, content):
        """AIé–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã©ã†ã‹åˆ¤å®š"""
        text = content.lower()
        ai_keywords = [
            'ai', 'artificial intelligence', 'machine learning', 'ml', 'deep learning',
            'chatgpt', 'gpt-4', 'gpt-5', 'claude', 'gemini', 'llm', 'openai', 'anthropic',
            'neural network', 'transformer', 'diffusion', 'rag', 'embedding',
            'äººå·¥çŸ¥èƒ½', 'AI', 'æ©Ÿæ¢°å­¦ç¿’', 'ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°', 'ç”ŸæˆAI', 'ãƒãƒ£ãƒƒãƒˆGPT',
            'pytorch', 'tensorflow', 'hugging face', 'langchain', 'vector database',
            'fine-tuning', 'prompt engineering', 'multimodal', 'computer vision'
        ]
        
        return any(keyword in text for keyword in ai_keywords)
    
    def extract_title(self, content):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ\n]', content)
        if sentences and len(sentences[0]) <= 80:
            return sentences[0].strip()
        return content[:50] + "..." if len(content) > 50 else content
    
    def parse_date(self, date_string):
        """æ—¥ä»˜æ–‡å­—åˆ—ã‚’è§£æ"""
        try:
            if 'T' in date_string:
                if date_string.endswith('Z'):
                    date_string = date_string[:-1] + '+00:00'
                dt = datetime.fromisoformat(date_string)
                if dt.tzinfo is not None:
                    dt = dt.replace(tzinfo=None)
                return dt.strftime('%Y-%m-%d')
            else:
                return datetime.now().strftime('%Y-%m-%d')
        except:
            return datetime.now().strftime('%Y-%m-%d')
    
    def generate_html(self, all_articles):
        """çµ±åˆè¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰HTMLã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½¿ç”¨ï¼‰"""
        print(f"Generating integrated HTML... ({len(all_articles)} articles total)")
        
        # è¨˜äº‹ã¯æ—¢ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼ˆå¿µã®ãŸã‚å†åº¦ã‚½ãƒ¼ãƒˆï¼‰
        all_articles.sort(key=lambda x: x.get('total_score', 0.0), reverse=True)
        
        # ä¸Šä½è¨˜äº‹ã®ã‚¹ã‚³ã‚¢ã‚’ç¢ºèª
        print(f"Top 3 articles by score:")
        for i, article in enumerate(all_articles[:3]):
            score = article.get('total_score', 0)
            print(f"{i+1}. {article['title'][:40]}... - Score: {score:.3f}")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ãªHTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
        template_path = self.project_root / "docs" / "index_clean.html"
        try:
            with open(template_path, 'r', encoding='utf-8') as f:
                template_content = f.read()
            
            # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’JSONã¨ã—ã¦æŒ¿å…¥
            articles_json = json.dumps(all_articles, ensure_ascii=False, indent=2)
            
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§ç½®æ›
            html_content = template_content.replace(
                'const articles = [];',
                f'const articles = {articles_json};'
            )
            
            return html_content
            
        except FileNotFoundError:
            print("Warning: Clean template not found, using fallback template")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ”¹å–„ç‰ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç›´æ¥ä½œæˆ
            articles_json = json.dumps(all_articles, ensure_ascii=False, indent=2)
            
            html_content = f"""<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Daily AI News â€“ æ”¹è‰¯ç‰ˆ</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <!-- Header with title and persona toggle -->
    <header class="header">
        <div class="container">
            <h1>Daily AI News</h1>
            <p>Xè¨˜äº‹ã¨RSSè¨˜äº‹ã‚’ã¾ã¨ã‚ãŸæœ€æ–°AIãƒ‹ãƒ¥ãƒ¼ã‚¹</p>
            <!-- Persona toggle integrated in header -->
            <div class="persona-toggle">
                <button class="active" data-persona="engineer">æŠ€è¡“è€…å‘ã‘</button>
                <button data-persona="business">ãƒ“ã‚¸ãƒã‚¹å‘ã‘</button>
            </div>
        </div>
    </header>

    <!-- Summary statistics -->
    <section class="summary-stats">
        <div class="container">
            <div class="stats-grid">
                <div class="stat-item">
                    <div class="value" id="stat-total">0</div>
                    <div class="label">è¨˜äº‹æ•°</div>
                </div>
                <div class="stat-item">
                    <div class="value" id="stat-avg-score">0%</div>
                    <div class="label">å¹³å‡ã‚¹ã‚³ã‚¢</div>
                </div>
                <div class="stat-item">
                    <div class="value" id="stat-tier1">0</div>
                    <div class="label">é«˜ä¿¡é ¼ã‚½ãƒ¼ã‚¹</div>
                </div>
            </div>
        </div>
    </section>

    <!-- Filter panel -->
    <section class="filters">
        <div class="container">
            <div class="filter-row">
                <div class="filter-group">
                    <label for="source-tier-filter">ã‚½ãƒ¼ã‚¹ç¨®åˆ¥</label>
                    <select id="source-tier-filter">
                        <option value="all">ã™ã¹ã¦</option>
                        <option value="1">é«˜ä¿¡é ¼ã‚½ãƒ¼ã‚¹</option>
                        <option value="2">ä¸€èˆ¬ã‚½ãƒ¼ã‚¹</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label for="min-score-filter">æœ€ä½ã‚¹ã‚³ã‚¢</label>
                    <input type="range" id="min-score-filter" min="0" max="1" step="0.1" value="0" />
                </div>
                <div class="search-box">
                    <input id="search-input" type="text" placeholder="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢â€¦" />
                </div>
            </div>
        </div>
    </section>

    <!-- Articles container -->
    <main>
        <div class="container">
            <div class="articles-grid" id="articles-container"></div>
        </div>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">Â© 2025 Daily AI News â€“ æ”¹è‰¯ç‰ˆ</div>
    </footer>

    <script>
        // Persona-specific metric labels
        const metricLabels = {{
            engineer: ['æŠ€è¡“çš„æ–°è¦æ€§', 'å®Ÿè£…å¯èƒ½æ€§', 'å†ç¾æ€§', 'å®Ÿå‹™å¯„ä¸', 'å­¦ç¿’ä¾¡å€¤'],
            business: ['äº‹æ¥­å½±éŸ¿åº¦', 'æŠ•è³‡åˆ¤æ–­ææ–™', 'æˆ¦ç•¥çš„ä¾¡å€¤', 'å®Ÿç¾å¯èƒ½æ€§', 'ãƒªã‚¹ã‚¯è©•ä¾¡']
        }};

        const breakdownOrder = {{
            engineer: ['temporal', 'relevance', 'trust', 'quality', 'actionability'],
            business: ['quality', 'relevance', 'trust', 'temporal', 'actionability']
        }};

        // Current real-time articles data
        const articles = {articles_json};

        let filteredArticles = [...articles];
        let currentPersona = 'engineer';

        // Human-readable names for source tiers
        const tierTexts = {{
            1: 'é«˜ä¿¡é ¼ã‚½ãƒ¼ã‚¹',
            2: 'ä¸€èˆ¬ã‚½ãƒ¼ã‚¹'
        }};

        function renderArticles() {{
            const container = document.getElementById('articles-container');
            container.innerHTML = '';
            
            // Sort by recommendation priority first then by score
            const recPriority = {{ must_read: 0, recommended: 1, consider: 2, skip: 3 }};
            const sorted = [...filteredArticles].sort((a, b) => {{
                const aRec = (a.evaluation && a.evaluation[currentPersona] && a.evaluation[currentPersona].recommendation) || 'consider';
                const bRec = (b.evaluation && b.evaluation[currentPersona] && b.evaluation[currentPersona].recommendation) || 'consider';
                if (recPriority[aRec] !== recPriority[bRec]) {{
                    return recPriority[aRec] - recPriority[bRec];
                }}
                return getPersonaScore(b) - getPersonaScore(a);
            }});
            
            // Group by recommendation and insert headings for clarity
            let lastRec = null;
            sorted.forEach(article => {{
                const evalData = article.evaluation && article.evaluation[currentPersona];
                const rec = (evalData && evalData.recommendation) || 'consider';
                if (rec !== lastRec) {{
                    const heading = document.createElement('h2');
                    heading.className = `rec-heading rec-${{rec}}`;
                    heading.textContent = getRecommendationText(rec);
                    container.appendChild(heading);
                    lastRec = rec;
                }}
                container.appendChild(createArticleCard(article));
            }});
        }}

        function createArticleCard(article) {{
            const card = document.createElement('div');
            card.className = 'article-card';
            const evaluation = article.evaluation || {{}};
            const personaEval = evaluation[currentPersona] || {{}};
            const breakdown = personaEval.breakdown || {{}};
            const totalPercentage = Math.round((personaEval.total_score || 0) * 100);
            
            // Determine the order of breakdown keys for this persona
            const order = breakdownOrder[currentPersona] || ['quality','relevance','temporal','trust','actionability'];
            
            // Build HTML for breakdown items in the desired order
            let breakdownHtml = '';
            order.forEach((key, idx) => {{
                const val = Math.round(((breakdown[key] || 0) * 100));
                const label = (metricLabels[currentPersona] && metricLabels[currentPersona][idx]) || key;
                breakdownHtml += `<div class="score-item"><div class="score-value">${{val}}</div><div class="score-label">${{label}}</div></div>`;
            }});
            
            card.innerHTML = `
                <span class="source-tier tier-${{article.source_tier}}">${{tierTexts[article.source_tier] || ''}}</span>
                <h3 class="article-title">
                    <a href="${{article.url}}" target="_blank" rel="noopener noreferrer">${{escapeHtml(article.title)}}</a>
                </h3>
                <div class="article-meta">
                    <span>${{escapeHtml(article.source)}}</span> â€¢ <span>${{article.published_date}}</span>
                </div>
                <div class="article-content">${{escapeHtml(article.content)}}</div>
                <div class="evaluation-panel">
                    <div class="score-display">
                        <div class="total-score">${{totalPercentage}}</div>
                        <div class="score-label-text">ç·åˆè©•ä¾¡</div>
                        <div class="score-bar"><div class="score-bar-fill" style="width: ${{totalPercentage}}%"></div></div>
                    </div>
                    <div class="score-breakdown">
                        ${{breakdownHtml}}
                    </div>
                    <div class="recommendation rec-${{personaEval.recommendation || 'consider'}}">
                        ${{getRecommendationText(personaEval.recommendation || 'consider')}}
                    </div>
                </div>
            `;
            return card;
        }}

        function escapeHtml(text) {{
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }}

        function getRecommendationText(rec) {{
            const texts = {{ must_read: 'å¿…èª­', recommended: 'æ¨å¥¨', consider: 'æ¤œè¨', skip: 'ã‚¹ã‚­ãƒƒãƒ—' }};
            return texts[rec] || rec;
        }}

        function getPersonaScore(article) {{
            const evalData = article.evaluation && article.evaluation[currentPersona];
            return evalData ? evalData.total_score : 0;
        }}

        function applyFilters() {{
            const tierFilter = document.getElementById('source-tier-filter').value;
            const minScore = parseFloat(document.getElementById('min-score-filter').value || 0);
            const searchQuery = document.getElementById('search-input').value.toLowerCase();
            
            filteredArticles = articles.filter(article => {{
                if (tierFilter !== 'all' && article.source_tier !== parseInt(tierFilter)) return false;
                if (getPersonaScore(article) < minScore) return false;
                if (searchQuery) {{
                    const haystack = (article.title + article.content + article.source).toLowerCase();
                    if (!haystack.includes(searchQuery)) return false;
                }}
                return true;
            }});
            
            renderArticles();
            updateSummaryStats();
        }}

        function updateSummaryStats() {{
            const totalEl = document.getElementById('stat-total');
            const avgEl = document.getElementById('stat-avg-score');
            const tier1El = document.getElementById('stat-tier1');
            
            totalEl.textContent = filteredArticles.length;
            const avgScore = filteredArticles.reduce((sum, art) => sum + getPersonaScore(art), 0) / (filteredArticles.length || 1);
            avgEl.textContent = Math.round(avgScore * 100) + '%';
            const tier1Count = filteredArticles.filter(a => a.source_tier === 1).length;
            tier1El.textContent = tier1Count;
        }}

        // Event listeners
        document.getElementById('source-tier-filter').addEventListener('change', applyFilters);
        document.getElementById('min-score-filter').addEventListener('input', applyFilters);
        document.getElementById('search-input').addEventListener('input', applyFilters);

        // Attach persona toggle events on header
        document.querySelectorAll('.header .persona-toggle button').forEach(btn => {{
            btn.addEventListener('click', () => {{
                // update active class
                document.querySelectorAll('.header .persona-toggle button').forEach(b => b.classList.remove('active'));
                btn.classList.add('active');
                // update global persona
                currentPersona = btn.dataset.persona;
                // re-apply filters and re-render
                applyFilters();
            }});
        }});

        // Initial render
        document.addEventListener('DOMContentLoaded', () => {{
            renderArticles();
            updateSummaryStats();
        }});
    </script>
</body>
</html>"""
            
            return html_content
    



    def save_html(self, html_content):
        """HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        output_path = self.docs_path / "index.html"
        
        try:
            self.docs_path.mkdir(exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8', newline='') as f:
                f.write(html_content)
            print(f"HTML file saved: {output_path}")
            return True
        except Exception as e:
            print(f"HTML save error: {str(e)}")
            return False
    
    def run(self):
        """çµ±åˆåé›†å®Ÿè¡Œ"""
        print("Starting integrated article collection...")
        
        # Xè¨˜äº‹ã‚’åé›†
        x_articles = self.collect_x_articles()
        
        # RSSè¨˜äº‹ã‚’åé›†
        rss_articles = self.collect_rss_articles()
        
        # çµ±åˆ
        all_articles = x_articles + rss_articles
        
        if not all_articles:
            print("No articles collected")
            return False
        
        # å…¨è¨˜äº‹ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’ç¢ºèªãƒ»è¨­å®š
        for article in all_articles:
            if 'total_score' not in article:
                engineer_eval = article.get('evaluation', {}).get('engineer', {})
                article['total_score'] = engineer_eval.get('total_score', 0.0)
        
        # ãƒ‡ãƒãƒƒã‚°: è©•ä¾¡ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
        print(f"\nArticle scores (before sorting):")
        for i, article in enumerate(all_articles[:5]):
            print(f"{i+1}. {article['title'][:50]}... - Score: {article.get('total_score', 0):.3f}")
        
        # è©•ä¾¡é †ã§ã‚½ãƒ¼ãƒˆ
        all_articles.sort(key=lambda x: x.get('total_score', 0.0), reverse=True)
        
        # ãƒ‡ãƒãƒƒã‚°: ã‚½ãƒ¼ãƒˆå¾Œã®ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
        print(f"\nArticle scores (after sorting):")
        for i, article in enumerate(all_articles[:5]):
            print(f"{i+1}. {article['title'][:50]}... - Score: {article.get('total_score', 0):.3f}")
        
        # HTMLç”Ÿæˆ
        html_content = self.generate_html(all_articles)
        
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        if self.save_html(html_content):
            print(f"\nIntegrated collection completed!")
            print(f"Total articles: {len(all_articles)}")
            print(f"X articles: {len(x_articles)}")
            print(f"RSS articles: {len(rss_articles)}")
            print("Please open docs/index.html in browser")
            return True
        
        return False


def main():
    collector = IntegratedCollector()
    success = collector.run()
    
    if success:
        print("\nIntegrated collection completed successfully")
    else:
        print("\nCollection failed")


if __name__ == "__main__":
    main()
